"use strict";(self.webpackChunkgrc=self.webpackChunkgrc||[]).push([[9001],{98032:e=>{e.exports=JSON.parse('{"abstract":"Scientific workflows increasingly need to train a DNN model in\\nreal-time during an experiment (e.g. using ground truth from a sim-\\nulation), while using it at the same time for inferences. Instead of\\nsharing the same model instance, the training (producer) and infer-\\nence server (consumer) often use different model replicas that are\\nkept synchronized. In addition to efficient I/O techniques to keep the\\nmodel replica of the producer and consumer synchronized, there is\\nanother important trade-off: frequent model updates enhance infer-\\nence quality but may slow down training; infrequent updates may\\nlead to less precise inference results. To address these challenges, we\\nintroduce Viper: a new I/O framework designed to determine a near-\\noptimal checkpoint schedule and accelerate the delivery of the latest\\nmodel updates. Viper builds an inference performance predictor to\\nidentify the optimal checkpoint schedule to balance the trade-off be-\\ntween training slowdown and inference quality improvement. It also\\ncreates a memory-first model transfer engine to accelerate model\\ndelivery through direct memory-to-memory communication. Our\\nexperiments show that Viper can reduce the model update latency\\nby \u2248 9x using the GPU-to-GPU data transfer engine and \u2248 3x using\\nthe DRAM-to-DRAM host data transfer. The checkpoint schedule\\nobtained from Viper\'s predictor also demonstrates improved cumu-\\nlative inference accuracy compared to the baseline of epoch-based\\nsolutions.","authors":["J. Ye","J. Cernuda","N. Rajesh","K. Bateman","O. Yildiz","T. Peterka","A. Nigmetov","D. Morozov","A. Kougkas","X.-H. Sun","B. Nicolae"],"date":"August, 2024","doi":"10.1145/3673038.3673070","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/ye2024viper.bib","citation":"http://cs.iit.edu/~scs/assets/files/ye2024viper.txt","pdf":"http://cs.iit.edu/~scs/assets/files/ye2024viper.pdf","slides":"http://cs.iit.edu/~scs/assets/files/ye2024viper.pptx"},"month":8,"slug":"ye-2024-viper-0a1b","tags":["AI Workflows","Adaptive AI Model Checkpointing","Coupled Training and Inferences","Inferences During Partial Training","LABIOS"],"title":"Viper: A High-Performance I/O Framework for Transparently Updating, Storing, and Transferring Deep Neural Network Models","type":"Conference","venue":"The 53th International Conference on Parallel Processing (ICPP\'24)","year":2024}')}}]);