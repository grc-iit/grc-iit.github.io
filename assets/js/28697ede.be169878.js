"use strict";(self.webpackChunkgrc=self.webpackChunkgrc||[]).push([[23415],{79014:e=>{e.exports=JSON.parse('[{"abstract":"HPC, Big Data Analytics, and Machine Learning have become in-\\ncreasingly intertwined as popular models such as LLMs and Diffusion\\nModels have been driving discovery in scientific fields. However,\\neach of these domains has its own storage infrastructure with unique\\nI/O interfaces and storage systems, requiring feature sets that are\\noften incompatible. Users with experience in one domain lack the\\nexpertise to change their applications to match the data stacks of\\nthe other domains, necessitating expensive conversions. There is\\na need for a transparent solution for the unification of disparate\\ndata stacks for the triple convergence of HPC, Big Data, and ML\\nthat can provide the required functionality while achieving higher\\nperformance. To better support converged HPC, Big Data, and ML\\nworkflows, this paper proposes DTIO, a scalable I/O runtime that\\nunifies the disparate I/O stack for modern scientific ML workflows.\\nDTIO utilizes a unique DataTask abstraction to express the move-\\nment of data, its ordering, and its dependencies on other data as a\\ntask. DTIO achieves a unification of scientific and ML workflows\\nby utilizing intelligent mapping of interfaces, and automatically de-\\ntermines the best method to relate their unique semantics. DTIO\'s\\nonline translation with DataTask caching can improve performance\\nby 49.6% compared to offline translation methods. DTIO also offers\\nnumerous optimizations, such as asynchronous I/O and aggregation.","authors":["K. Bateman","N. Rajesh","J. Cernuda","L. Logan","B. Nicolae","F. Cappello","X.-H. Sun","A. Kougkas"],"date":"June, 2025","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/bateman2025dtio.bib","citation":"http://cs.iit.edu/~scs/assets/files/bateman2025dtio.txt","pdf":"http://cs.iit.edu/~scs/assets/files/bateman2025dtio.pdf"},"month":6,"slug":"bateman-2025-dtio-e77d","tags":["Task Systems","Data Stacks","Systems for AI Workflows"],"title":"DTIO: Data Stack for AI-driven Workflows","type":"Conference","venue":"The 37th International Conference on Scalable Scientific Data Management (SSDBM 2025)","year":2025},{"abstract":"Scientific workflows increasingly need to train a DNN model in\\nreal-time during an experiment (e.g. using ground truth from a sim-\\nulation), while using it at the same time for inferences. Instead of\\nsharing the same model instance, the training (producer) and infer-\\nence server (consumer) often use different model replicas that are\\nkept synchronized. In addition to efficient I/O techniques to keep the\\nmodel replica of the producer and consumer synchronized, there is\\nanother important trade-off: frequent model updates enhance infer-\\nence quality but may slow down training; infrequent updates may\\nlead to less precise inference results. To address these challenges, we\\nintroduce Viper: a new I/O framework designed to determine a near-\\noptimal checkpoint schedule and accelerate the delivery of the latest\\nmodel updates. Viper builds an inference performance predictor to\\nidentify the optimal checkpoint schedule to balance the trade-off be-\\ntween training slowdown and inference quality improvement. It also\\ncreates a memory-first model transfer engine to accelerate model\\ndelivery through direct memory-to-memory communication. Our\\nexperiments show that Viper can reduce the model update latency\\nby \u2248 9x using the GPU-to-GPU data transfer engine and \u2248 3x using\\nthe DRAM-to-DRAM host data transfer. The checkpoint schedule\\nobtained from Viper\'s predictor also demonstrates improved cumu-\\nlative inference accuracy compared to the baseline of epoch-based\\nsolutions.","authors":["J. Ye","J. Cernuda","N. Rajesh","K. Bateman","O. Yildiz","T. Peterka","A. Nigmetov","D. Morozov","A. Kougkas","X.-H. Sun","B. Nicolae"],"date":"August, 2024","doi":"10.1145/3673038.3673070","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/ye2024viper.bib","citation":"http://cs.iit.edu/~scs/assets/files/ye2024viper.txt","pdf":"http://cs.iit.edu/~scs/assets/files/ye2024viper.pdf","slides":"http://cs.iit.edu/~scs/assets/files/ye2024viper.pptx"},"month":8,"slug":"ye-2024-viper-0a1b","tags":["AI Workflows","Adaptive AI Model Checkpointing","Coupled Training and Inferences","Inferences During Partial Training","LABIOS"],"title":"Viper: A High-Performance I/O Framework for Transparently Updating, Storing, and Transferring Deep Neural Network Models","type":"Conference","venue":"The 53th International Conference on Parallel Processing (ICPP\'24)","year":2024},{"abstract":"I/O operations are a known performance bottleneck of\\nHPC applications. To achieve good performance, users often employ\\nan iterative multistage tuning process to find an optimal I/O stack\\nconfiguration. However, an I/O stack contains multiple layers, such\\nas high-level I/O libraries, I/O middleware, and parallel file systems,\\nand each layer has many parameters. These parameters and layers\\nare entangled and influenced by each other. The tuning process is\\ntime-consuming and complex. In this work, we present TunIO, an AI-\\npowered I/O tuning framework that implements several techniques to\\nbalance the tuning cost and performance gain, including tuning the high-\\nimpact parameters first. Furthermore, TunIO analyzes the application\\nsource code to extract its I/O kernel while retaining all statements\\nnecessary to perform I/O. It utilizes a smart selection of high-impact\\nconfiguration parameters of the given tuning objective. Finally, it uses a\\nnovel Reinforcement Learning (RL)-driven early stopping mechanism to\\nbalance the cost and performance gain. Experimental results show that\\nTunIO leads to a reduction of up to \u2248 73% in tuning time while achieving\\nthe same performance gain when compared to H5Tuner. It achieves a\\nsignificant performance gain/cost of 208.4 MBps/min (I/O bandwidth for\\neach minute spent in tuning) over existing approaches under our testing.\\nIndex Terms-AI-powered I/O tuning, storage stack tuning, autotun-\\ning, source code transformations, and I/O performance optimization","authors":["N. Rajesh","K. Bateman","S. Byna","J. L. Bez","A. Kougkas","X.-H. Sun"],"date":"May, 2024","doi":"10.1109/ipdps57955.2024.00050","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/rajesh2024tunio.bib","citation":"http://cs.iit.edu/~scs/assets/files/rajesh2024tunio.txt","pdf":"http://cs.iit.edu/~scs/assets/files/rajesh2024tunio.pdf"},"month":5,"slug":"rajesh-2024-tunio-05a8","tags":["AI for I/O","I/O Stack Tuning"],"title":"TunIO: An AI-powered Framework for Optimizing HPC I/O","type":"Conference","venue":"The 38th IEEE International Parallel & Distributed Processing Symposium (IPDPS 2024)","year":2024},{"abstract":"Abstract-Storage in HPC is typically a single Remote and Static\\nStorage (RSS) resource. However, applications demonstrate diverse I/O\\nrequirements that can be better served by a multi-storage approach. Cur-\\nrent practice employs ephemeral storage systems running on either node-\\nlocal or shared storage resources. Yet, the burden of provisioning and\\nconfiguring intermediate storage falls solely on the users, while global job\\nschedulers offer little to no support for custom deployments. This lack of\\nsupport often leads to over- or under-provisioning of resources and poorly\\nconfigured storage systems. To mitigate this, we present LuxIO, an intelli-\\ngent storage resource provisioning and auto-configuration service. LuxIO\\nconstructs storage deployments configured to best match I/O require-\\nments. LuxIO-tuned storage services show performance improvements\\nup to 2x across common applications and benchmarks, while introducing\\nminimal overhead of 93.40 ms on top of existing job scheduling pipelines.\\nLuxIO improves resource utilization by up to 25% in select workflows.\\n","authors":["K. Bateman","N. Rajesh","J. Cernuda","L. Logan","J. Ye","S. Herbein","A. Kougkas","X.-H. Sun"],"date":"December, 2022","doi":"10.1109/hipc56025.2022.00041","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/bateman2022luxio.bib","citation":"http://cs.iit.edu/~scs/assets/files/bateman2022luxio.txt","pdf":"http://cs.iit.edu/~scs/assets/files/bateman2022luxio.pdf"},"month":12,"slug":"bateman-2022-luxio-2487","tags":["Resource Provisioning","I/O Behavior","Storage Auto-Tuning","ChronoLog"],"title":"LuxIO: Intelligent Resource Provisioning and Auto-Configuration for Storage Services","type":"Conference","venue":"The 29th edition of the IEEE International Conference on High Performance Computing, Data, and Analytics (HiPC\'22)","year":2022},{"authors":["N. Rajesh","Q. Koziol","S. Byna","H. Tang","J. L. Bez","A. Kougkas","X.-H. Sun"],"date":"November, 2021","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/rajesh2021features_poster.bib","citation":"http://cs.iit.edu/~scs/assets/files/rajesh2021features_poster.txt","extended abstract":"http://cs.iit.edu/~scs/assets/files/rajesh2021features_abstract.pdf","poster":"http://cs.iit.edu/~scs/assets/files/rajesh2021features_poster.pdf"},"month":11,"slug":"rajesh-2021-feature-reduction-4823","tags":["Feature Reduction","Evolutionary Algorithms","Darshan"],"title":"Feature Reduction of Darshan Counters Using Evolutionary Algorithms","type":"Poster","venue":"The 2021 International Conference for High Performance Computing, Networking, Storage, and Analysis (SC\'21), November 14\u201319, 2021","year":2021},{"abstract":"Modern applications are highly data-intensive, leading\\nto the well-known I/O bottleneck problem. Scientists have proposed\\nthe placement of fast intermediate storage resources which aim\\nto mask the I/O penalties. To manage these resources, three core\\nsoftware abstractions are being used in leadership-class computing\\nfacilities: IO Forwarders, Burst Buffers, and Data Stagers. Yet, with\\nthe rise of multi-tenant deployment in HPC systems, these software\\nabstractions are: managed and maintained in isolation, leading to\\ninefficient interactions; allocated statically, leading to load imbalance;\\nexclusively bifurcated between the intermediate storage, leading to\\nunder-utilization of resources, and, in many cases, do not support\\nin-situ operations. To this end, we present HFlow, a new class of\\ndata forwarding system that leverages a real-time data movement\\nparadigm. HFlow introduces a unified data movement abstraction\\n(the ByteFlow) providing data-independent tasks that can be\\nexecuted anywhere and thus, enabling dynamic resource provisioning.\\nMoreover, the processing elements executing the ByteFlows are\\ndesigned to be ephemeral and, hence, enable elastic management of\\nintermediate storage resources. Our results show that applications\\nrunning under HFlow display an increase in performance of 3x when\\ncompared with state-of-the-art software solutions.\\nIndex Terms-Data streaming, I/O forwarding, elasticity,\\ndynamicity, multi-tenant, data-intensive, I/O,data pipeline, in-transit","authors":["J. Cernuda","H. Devarajan","L. Logan","K. Bateman","N. Rajesh","J. Ye","A. Kougkas","X.-H. Sun"],"date":"September, 2021","doi":"10.1109/cluster48925.2021.00064","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/cernuda2021HFlow.bib","citation":"http://cs.iit.edu/~scs/assets/files/cernuda2021HFlow.txt","pdf":"http://cs.iit.edu/~scs/assets/files/cernuda2021HFlow.pdf"},"month":9,"slug":"cernuda-2021-hflow-2f5b","tags":["Hermes"],"title":"HFlow: A Dynamic and Elastic Multi-Layered Data Forwarder","type":"Conference","venue":"The 2021 IEEE International Conference on Cluster Computing (CLUSTER\'21), September 7-10, 2021","year":2021},{"abstract":"Applications and middleware services, such as data placement en-\\ngines, I/O scheduling, and prefetching engines, require low-latency\\naccess to telemetry data in order to make optimal decisions. However,\\ntypical monitoring services store their telemetry data in a database\\nin order to allow applications to query them, resulting in significant\\nlatency penalties. This work presents Apollo: a low-latency mon-\\nitoring service that aims to provide applications and middleware\\nlibraries with direct access to relational telemetry data. Monitoring\\nthe system can create interference and overhead, slowing down raw\\nperformance of the resources for the job. However, having a current\\nview of the system can aid middleware services in making more\\noptimal decisions which can ultimately improve the overall perfor-\\nmance. Apollo has been designed from the ground up to provide\\nlow latency, using Publish\u2013Subscribe (Pub-Sub) semantics, and low\\noverhead, using adaptive intervals in order to change the length of\\ntime between polling the resource for telemetry data and machine\\nlearning in order to predict changes to the telemetry data between\\nactual resource polling. This work also provides some high level\\nabstractions called I/O curators, which can further aid middleware\\nlibraries and applications to make optimal decisions. Evaluations\\nshowcase that Apollo can achieve sub-millisecond latency for acquir-\\ning complex insights with a memory overhead of ~57MB and CPU\\noverhead being only 7% more than existing state-of-the-art systems.","authors":["N. Rajesh","H. Devarajan","J. Cernuda","K. Bateman","L. Logan","J. Ye","A. Kougkas","X.-H. Sun"],"date":"June, 2021","doi":"10.1145/3431379.3460640","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/rajesh2021apollo.bib","citation":"http://cs.iit.edu/~scs/assets/files/rajesh2021apollo.txt","pdf":"http://cs.iit.edu/~scs/assets/files/rajesh2021apollo.pdf"},"month":6,"slug":"rajesh-2021-apollo-dbd8","tags":["HPC","Machine Learning","Resource Monitoring","Hermes"],"title":"Apollo: An ML-assisted Real-Time Storage Resource Observer","type":"Conference","venue":"The 30th ACM International Symposium on High-Performance Parallel and Distributed Computing (HPDC\'21), June 21-25, 2021","year":2021},{"authors":["N. Rajesh","G. Heber","A. Kougkas","X.-H. Sun"],"date":"November, 2020","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/rajesh2020characterizing.bib","citation":"http://cs.iit.edu/~scs/assets/files/rajesh2020characterizing.txt","extended abstract":"http://cs.iit.edu/~scs/assets/files/rajesh2020characterizing-abstract.pdf","poster":"http://cs.iit.edu/~scs/assets/files/rajesh2020characterizing-poster.pdf"},"month":11,"slug":"rajesh-2020-characterizing-approximating-1de1","tags":[],"title":"Characterizing and Approximating I/O Behavior of HDF5 Applications","type":"Poster","venue":"The International Conference for High Performance Computing, Networking, Storage and Analysis (SC\'20)","year":2020},{"abstract":"Modern applications produce and process massive\\namounts of activity (or log) data. Traditional storage systems\\nwere not designed with an append-only data model and a\\nnew storage abstraction aims to fill this gap: the distributed\\nshared log store. However, existing solutions struggle to provide\\na scalable, parallel, and high-performance solution that can\\nsupport a diverse set of conflicting log workload requirements.\\nFinding the tail of a distributed log is a centralized point of\\ncontention. In this paper, we show how using physical time can\\nhelp alleviate the need of centralized synchronization points. We\\npresent ChronoLog, a new, distributed, shared, and multi-tiered\\nlog store that can handle more than a million tail operations\\nper second. Evaluation results show ChronoLog\'s potential,\\noutperforming existing solution by an order of magnitude.","authors":["A. Kougkas","H. Devarajan","K. Bateman","J. Cernuda","N. Rajesh","X.-H. Sun"],"date":"October, 2020","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/chronolog_temp.bib","citation":"http://cs.iit.edu/~scs/assets/files/chronolog_temp.txt","pdf":"http://cs.iit.edu/~scs/assets/files/kougkas2020chronolog.pdf"},"month":10,"slug":"kougkas-2020-chronolog-c458","tags":["Distributed Log","Shared Log","Tiered Storage","ChronoLog"],"title":"ChronoLog: A Distributed Shared Tiered Log Store with Time-based Data Ordering","type":"Conference","venue":"The 36th International Conference on Massive Storage Systems and Technology (MSST\'20), Oct. 29-30, 2020","year":2020}]')}}]);