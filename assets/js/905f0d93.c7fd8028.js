"use strict";(self.webpackChunkgrc=self.webpackChunkgrc||[]).push([[85525],{92478:e=>{e.exports=JSON.parse('[{"abstract":"HPC, Big Data Analytics, and Machine Learning have become in-\\ncreasingly intertwined as popular models such as LLMs and Diffusion\\nModels have been driving discovery in scientific fields. However,\\neach of these domains has its own storage infrastructure with unique\\nI/O interfaces and storage systems, requiring feature sets that are\\noften incompatible. Users with experience in one domain lack the\\nexpertise to change their applications to match the data stacks of\\nthe other domains, necessitating expensive conversions. There is\\na need for a transparent solution for the unification of disparate\\ndata stacks for the triple convergence of HPC, Big Data, and ML\\nthat can provide the required functionality while achieving higher\\nperformance. To better support converged HPC, Big Data, and ML\\nworkflows, this paper proposes DTIO, a scalable I/O runtime that\\nunifies the disparate I/O stack for modern scientific ML workflows.\\nDTIO utilizes a unique DataTask abstraction to express the move-\\nment of data, its ordering, and its dependencies on other data as a\\ntask. DTIO achieves a unification of scientific and ML workflows\\nby utilizing intelligent mapping of interfaces, and automatically de-\\ntermines the best method to relate their unique semantics. DTIO\'s\\nonline translation with DataTask caching can improve performance\\nby 49.6% compared to offline translation methods. DTIO also offers\\nnumerous optimizations, such as asynchronous I/O and aggregation.","authors":["K. Bateman","N. Rajesh","J. Cernuda","L. Logan","B. Nicolae","F. Cappello","X.-H. Sun","A. Kougkas"],"date":"June, 2025","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/bateman2025dtio.bib","citation":"http://cs.iit.edu/~scs/assets/files/bateman2025dtio.txt","pdf":"http://cs.iit.edu/~scs/assets/files/bateman2025dtio.pdf"},"month":6,"slug":"bateman-2025-dtio-e77d","tags":["Task Systems","Data Stacks","Systems for AI Workflows"],"title":"DTIO: Data Stack for AI-driven Workflows","type":"Conference","venue":"The 37th International Conference on Scalable Scientific Data Management (SSDBM 2025)","year":2025},{"abstract":"Pre-training of LLMs and transformers is known to\\ntake weeks if not months even with powerful HPC systems. However,\\ninferences are an equally important problem: once pre-trained, the\\nmodel needs to serve a large number of inferences submitted under\\nconcurrency by multiple users. Thus, speeding up each inference\\nrequest is instrumental in achieving high throughput and latency at\\nscale. To avoid redundant recomputation in each decode iteration, a\\nKey-Value (KV) cache is used to store previously computed keys (K)\\nand values (V), speeding up token generation. GPU memory is\\nprimarily consumed by model weights and the remainder is used by\\nthe KV cache. Thus, the free GPU space available to the KV cache\\nis a scarce resource that needs to be managed in an efficient way in\\norder to minimize the overhead of redundant recomputations. There\\nare many optimizations applied in this context: batching of inference\\nrequests to enable them to run in the same forward pass (and thus\\nincrease the parallelism and inference throughput), different KV\\ncache eviction policies (simply drop KV entries and recompute them\\nlater vs. swap to host memory), etc. Under these circumstances, the\\ndecision of what batching strategy, what KV cache eviction policy\\nto apply and how the KV cache impacts the inference performance\\nis non-trivial. Unlike the case of pre-training, state-of-art studies\\nare scarce in this context. To fill this gap, in this paper we study\\nthe impact of KV caching. Specifically, we instrument vLLM to\\nmeasure and analyze fine-grain metrics (token throughput, KV\\ncache memory access patterns, load balancing of the forward passes),\\nduring different inference stages (prefill, decode) in several scenarios\\nthat involve concurrent inference requests using several benchmarks.\\nBased on the measurements and associated observations, we identify\\nseveral opportunities to improve the design of inference frameworks.\\nIndex Terms-LLM inference, KV cache profiling, access pattern\\ncharacterization.","authors":["J. Ye","J. Cernuda","A. Maurya","X.-H. Sun","A. Kougkas","B. Nicolae"],"date":"June, 2025","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/ye2025unboxkv_io.bib","citation":"http://cs.iit.edu/~scs/assets/files/ye2025unboxkv_io.txt","pdf":"http://cs.iit.edu/~scs/assets/files/ye2025unboxkv_io.pdf"},"month":6,"slug":"ye-2025-characterizing-behavior-f631","tags":["LABIOS"],"title":"Characterizing the Behavior and Impact of KV Caching on Transformer Inferences under Concurrency","type":"Conference","venue":"The 39th IEEE International Parallel & Distributed Processing Symposium (IPDPS 2025)","year":2025},{"abstract":"Hardware is becoming increasingly heterogeneous in\\nmodern high-performance computing clusters. However, computing\\nenvironments for developing tools to harness these technologies are\\nnot easily available to researchers. This work showcases the need for\\na new high-pace, heterogeneous I/O research cluster and presents a\\nnovel software deployment framework named Jarvis to manage its\\nhardware diversity. Jarvis is an extensible Python framework that\\nallows users to create Packages that deploy, manage, and monitor\\nsoftware, including complex applications (e.g., scientific simulations),\\nsupport tools (e.g., Darshan, GDB), and storage systems (e.g.,\\nLustre, DAOS). These packages can be combined to form complex\\ndeployment Pipelines. To ensure pipelines are portable across\\nhardware, Jarvis defines a novel Resource Graph schema file,\\nwhich is a snapshot of a cluster\'s machine-specific information.\\nThis schema can be queried by Jarvis packages to deploy software\\nacross diverse hardware compositions with minimal user effort.\\nIndex Terms-deployment, HPC, hardware abstraction, resource\\nmanagement, I/O, Python","authors":["J. Cernuda","L. Logan","N. Lewis","S. Byna","X.-H. Sun","A. Kougkas"],"date":"November, 2024","links":{"pdf":"http://cs.iit.edu/~scs/assets/files/cernuda2024jarvis.pdf"},"month":11,"slug":"cernuda-2024-jarvis-3b52","tags":["Deployment","HPC","Hardware Abstraction","I/O","Python","Resource Management"],"title":"Jarvis: Towards a Shared, User-Friendly, and Reproducible, I/O Infrastructure.","type":"Workshop","venue":"The International Parallel Data Systems Workshop (PDSW\'24)","year":2024},{"abstract":"The combination of ever-growing scientific datasets\\nand distributed workflow complexity creates I/O performance\\nbottlenecks due to data volume, velocity, and variety. Although\\nthe increasing use of descriptive data formats (e.g., HDF5,\\nnetCDF) helps organize these datasets, it also introduces obscure\\nbottlenecks due to the need to translate high-level operations\\ninto file addresses and then into low-level I/O operations. To\\naddress this challenge, we introduce DaYu, a method and toolset\\nfor analyzing (a) semantic relationships between logical datasets\\nand file addresses, (b) how dataset operations translate into\\nI/O, and (c) the combination across entire workflows. DaYu\'s\\nanalysis and visualization enable the identification of critical\\nbottlenecks and the reasoning about remediation. We describe\\nour methodology and propose optimization guidelines. Evaluation\\non scientific workflows demonstrates up to a 3.7x performance\\nimprovement in I/O time for obscure bottlenecks. The time and\\nstorage overhead for DaYu\'s time-ordered data are typically\\nunder 0.2% of runtime and 0.25% of data volume, respectively.","authors":["M. Tang","J. Cernuda","J. Ye","L. Guo","N. Tallent","A. Kougkas","X.-H. Sun"],"date":"September, 2024","doi":"10.1109/cluster59578.2024.00038","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/tang2024dayu.bib","citation":"http://cs.iit.edu/~scs/assets/files/tang2024dayu.txt","pdf":"http://cs.iit.edu/~scs/assets/files/tang2024dayu.pdf"},"month":9,"slug":"tang-2024-dayu-f286","tags":["Data Analytics","Modeling","Performance Measurement","Tools","Workflow Optimization"],"title":"DaYu: Optimizing Distributed Scientific Workflows by Decoding Dataflow Semantics and Dynamics","type":"Conference","venue":"2024 IEEE International Conference on Cluster Computing (CLUSTER\'24)","year":2024},{"abstract":"Data streaming is gaining traction in high-performance computing\\n(HPC) as a mechanism for continuous data transfer, but remains\\nunderutilized as a processing paradigm due to the inadequacy of\\nexisting technologies, which are primarily designed for cloud archi-\\ntectures and ill-equipped to tackle HPC-specific challenges. This\\nwork introduces HStream, a novel data management design for\\nout-of-core data streaming engines. Central to the HStream design\\nis the separation of data and computing planes at the task level. By\\nmanaging them independently, issues such as memory thrashing\\nand back-pressure, caused by the high volume, velocity, and bursti-\\nness of I/O in HPC environments, can be effectively addressed at\\nruntime. Specifically, HStream utilizes adaptive parallelism and hi-\\nerarchical memory management, enabled by this design paradigm,\\nto alleviate memory pressure and enhance system performance.\\nThese improvements enable HStream to match the performance of\\nstate-of-the-art HPC streaming engines and achieve up to a 1.5x\\nreduction in latency under high data loads.","authors":["J. Cernuda","J. Ye","A. Kougkas","X.-H. Sun"],"date":"August, 2024","doi":"10.1145/3673038.3673150","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/cernuda2024hstream.bib","citation":"http://cs.iit.edu/~scs/assets/files/cernuda2024hstream.txt","pdf":"http://cs.iit.edu/~scs/assets/files/cernuda2024hstream.pdf"},"month":8,"slug":"cernuda-2024-hstream-3043","tags":["Data Streaming","HPC","Hierarchical Storage","Elastic System","In-Transit Computing","LABIOS"],"title":"HStream: A hierarchical data streaming engine for high-throughput scientific applications","type":"Conference","venue":"The 53th International Conference on Parallel Processing (ICPP\'24)","year":2024},{"abstract":"Scientific workflows increasingly need to train a DNN model in\\nreal-time during an experiment (e.g. using ground truth from a sim-\\nulation), while using it at the same time for inferences. Instead of\\nsharing the same model instance, the training (producer) and infer-\\nence server (consumer) often use different model replicas that are\\nkept synchronized. In addition to efficient I/O techniques to keep the\\nmodel replica of the producer and consumer synchronized, there is\\nanother important trade-off: frequent model updates enhance infer-\\nence quality but may slow down training; infrequent updates may\\nlead to less precise inference results. To address these challenges, we\\nintroduce Viper: a new I/O framework designed to determine a near-\\noptimal checkpoint schedule and accelerate the delivery of the latest\\nmodel updates. Viper builds an inference performance predictor to\\nidentify the optimal checkpoint schedule to balance the trade-off be-\\ntween training slowdown and inference quality improvement. It also\\ncreates a memory-first model transfer engine to accelerate model\\ndelivery through direct memory-to-memory communication. Our\\nexperiments show that Viper can reduce the model update latency\\nby \u2248 9x using the GPU-to-GPU data transfer engine and \u2248 3x using\\nthe DRAM-to-DRAM host data transfer. The checkpoint schedule\\nobtained from Viper\'s predictor also demonstrates improved cumu-\\nlative inference accuracy compared to the baseline of epoch-based\\nsolutions.","authors":["J. Ye","J. Cernuda","N. Rajesh","K. Bateman","O. Yildiz","T. Peterka","A. Nigmetov","D. Morozov","A. Kougkas","X.-H. Sun","B. Nicolae"],"date":"August, 2024","doi":"10.1145/3673038.3673070","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/ye2024viper.bib","citation":"http://cs.iit.edu/~scs/assets/files/ye2024viper.txt","pdf":"http://cs.iit.edu/~scs/assets/files/ye2024viper.pdf","slides":"http://cs.iit.edu/~scs/assets/files/ye2024viper.pptx"},"month":8,"slug":"ye-2024-viper-0a1b","tags":["AI Workflows","Adaptive AI Model Checkpointing","Coupled Training and Inferences","Inferences During Partial Training","LABIOS"],"title":"Viper: A High-Performance I/O Framework for Transparently Updating, Storing, and Transferring Deep Neural Network Models","type":"Conference","venue":"The 53th International Conference on Parallel Processing (ICPP\'24)","year":2024},{"abstract":"Modern simulation workflows generate and analyze\\nmassive amounts of data using I/O libraries like Adios2 and\\nNetCDF. Although extensive work has optimized the I/O\\nprocesses during the simulation phase, executing analytical\\nqueries-which often require iterative traversals of large files\\nfor insights-is cumbersome and usually constrained by low I/O\\nperformance. Instead of waiting for the analysis phase to process\\nqueries, quantities can be derived asynchronously during data\\nproduction and cached, speeding up future queries. In this\\nwork, we introduce a context-aware I/O layer named \'Hades.\' It\\nis designed to efficiently derive insights from selected quantities\\nwithout compromising overall workflow performance. Hades\\nactively and asynchronously computes and stores these quantities\\nwhile the data is in transit. Hades leverages a hierarchical\\nbuffering system with data access-aware prefetching to ensure\\nquick and timely access to relevant data. It offers a flexible\\nquery interface empowering users to easily define derived\\nquantities and provide control over data placement decisions.\\nHades is implemented using an Adios2 plugin engine and the\\nHermes buffering platform, enabling transparent use by any\\nAdios-powered application or workflow. Experimental results\\ndemonstrate performance improvements by up to 3-4x for tested\\nreal-world scientific producer-consumer workflows.\\nIndex Terms-Active Storage, Hierarchical Storage, Context\\nAwareness, Metadata Management, Data Operator, In-transit\\nComputing","authors":["J. Cernuda","L. Logan","A. Gainaru","J. Lofstead","A. Kougkas","X.-H. Sun"],"date":"May, 2024","doi":"10.1109/ccgrid59990.2024.00070","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/cernuda2024hades.bib","citation":"http://cs.iit.edu/~scs/assets/files/cernuda2024hades.txt","pdf":"http://cs.iit.edu/~scs/assets/files/cernuda2024hades.pdf"},"month":5,"slug":"cernuda-2024-hades-e18c","tags":["Active Storage","Hierarchical Storage","Context Awareness","Metadata Management","Data Operator","In-Transit Computing","Coeus"],"title":"Hades: A Context-Aware Active Storage Framework for Accelerating Large-Scale Data Analysis","type":"Conference","venue":"The 24th IEEE/ACM international Symposium on Cluster, Cloud and Internet Computing (CCGRID 2024)","year":2024},{"abstract":"Abstract-Storage in HPC is typically a single Remote and Static\\nStorage (RSS) resource. However, applications demonstrate diverse I/O\\nrequirements that can be better served by a multi-storage approach. Cur-\\nrent practice employs ephemeral storage systems running on either node-\\nlocal or shared storage resources. Yet, the burden of provisioning and\\nconfiguring intermediate storage falls solely on the users, while global job\\nschedulers offer little to no support for custom deployments. This lack of\\nsupport often leads to over- or under-provisioning of resources and poorly\\nconfigured storage systems. To mitigate this, we present LuxIO, an intelli-\\ngent storage resource provisioning and auto-configuration service. LuxIO\\nconstructs storage deployments configured to best match I/O require-\\nments. LuxIO-tuned storage services show performance improvements\\nup to 2x across common applications and benchmarks, while introducing\\nminimal overhead of 93.40 ms on top of existing job scheduling pipelines.\\nLuxIO improves resource utilization by up to 25% in select workflows.\\n","authors":["K. Bateman","N. Rajesh","J. Cernuda","L. Logan","J. Ye","S. Herbein","A. Kougkas","X.-H. Sun"],"date":"December, 2022","doi":"10.1109/hipc56025.2022.00041","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/bateman2022luxio.bib","citation":"http://cs.iit.edu/~scs/assets/files/bateman2022luxio.txt","pdf":"http://cs.iit.edu/~scs/assets/files/bateman2022luxio.pdf"},"month":12,"slug":"bateman-2022-luxio-2487","tags":["Resource Provisioning","I/O Behavior","Storage Auto-Tuning","ChronoLog"],"title":"LuxIO: Intelligent Resource Provisioning and Auto-Configuration for Storage Services","type":"Conference","venue":"The 29th edition of the IEEE International Conference on High Performance Computing, Data, and Analytics (HiPC\'22)","year":2022},{"abstract":"Traditionally, I/O systems have been developed within\\nthe confines of a centralized OS kernel. This led to monolithic\\nand rigid storage systems that are limited by low development\\nspeed, expressiveness, and performance. Various assumptions are\\nimposed including reliance on the UNIX-file abstraction, the POSIX\\nstandard, and a narrow set of I/O policies. However, this monolithic\\ndesign philosophy makes it difficult to develop and deploy new\\nI/O approaches to satisfy the rapidly-evolving I/O requirements of\\nmodern scientific applications. To this end, we propose LabStor: a\\nmodular and extensible platform for developing high-performance,\\ncustomized I/O stacks. Single-purpose I/O modules (e.g, I/O\\nschedulers) can be developed in the comfort of userspace and released\\nas plug-ins, while end-users can compose these modules to form\\nworkload- and hardware-specific I/O stacks. Evaluations show that\\nby switching to a fully modular design, tailored I/O stacks can yield\\nperformance improvements of up to 60% in various applications.","authors":["L. Logan","J. Cernuda","J. Lofstead","X.-H. Sun","A. Kougkas"],"date":"November, 2022","doi":"10.1109/sc41404.2022.00028","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/logan2022labstor.bib","citation":"http://cs.iit.edu/~scs/assets/files/logan2022labstor.txt","pdf":"http://cs.iit.edu/~scs/assets/files/logan2022labstor.pdf","slides":"http://cs.iit.edu/~scs/assets/files/logan2022labstor_slides.pdf"},"month":11,"slug":"logan-2022-labstor-f69d","tags":["Clouds and Distributed Computing","Programming Frameworks","System Software","ChronoLog"],"title":"LabStor: A Modular and Extensible Platform for Developing High-Performance, Customized I/O Stacks in Userspace","type":"Conference","venue":"The International Conference for High Performance Computing, Networking, Storage, and Analysis (SC\'22)","year":2022},{"abstract":"Modern applications are highly data-intensive, leading\\nto the well-known I/O bottleneck problem. Scientists have proposed\\nthe placement of fast intermediate storage resources which aim\\nto mask the I/O penalties. To manage these resources, three core\\nsoftware abstractions are being used in leadership-class computing\\nfacilities: IO Forwarders, Burst Buffers, and Data Stagers. Yet, with\\nthe rise of multi-tenant deployment in HPC systems, these software\\nabstractions are: managed and maintained in isolation, leading to\\ninefficient interactions; allocated statically, leading to load imbalance;\\nexclusively bifurcated between the intermediate storage, leading to\\nunder-utilization of resources, and, in many cases, do not support\\nin-situ operations. To this end, we present HFlow, a new class of\\ndata forwarding system that leverages a real-time data movement\\nparadigm. HFlow introduces a unified data movement abstraction\\n(the ByteFlow) providing data-independent tasks that can be\\nexecuted anywhere and thus, enabling dynamic resource provisioning.\\nMoreover, the processing elements executing the ByteFlows are\\ndesigned to be ephemeral and, hence, enable elastic management of\\nintermediate storage resources. Our results show that applications\\nrunning under HFlow display an increase in performance of 3x when\\ncompared with state-of-the-art software solutions.\\nIndex Terms-Data streaming, I/O forwarding, elasticity,\\ndynamicity, multi-tenant, data-intensive, I/O,data pipeline, in-transit","authors":["J. Cernuda","H. Devarajan","L. Logan","K. Bateman","N. Rajesh","J. Ye","A. Kougkas","X.-H. Sun"],"date":"September, 2021","doi":"10.1109/cluster48925.2021.00064","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/cernuda2021HFlow.bib","citation":"http://cs.iit.edu/~scs/assets/files/cernuda2021HFlow.txt","pdf":"http://cs.iit.edu/~scs/assets/files/cernuda2021HFlow.pdf"},"month":9,"slug":"cernuda-2021-hflow-2f5b","tags":["Hermes"],"title":"HFlow: A Dynamic and Elastic Multi-Layered Data Forwarder","type":"Conference","venue":"The 2021 IEEE International Conference on Cluster Computing (CLUSTER\'21), September 7-10, 2021","year":2021},{"abstract":"Applications and middleware services, such as data placement en-\\ngines, I/O scheduling, and prefetching engines, require low-latency\\naccess to telemetry data in order to make optimal decisions. However,\\ntypical monitoring services store their telemetry data in a database\\nin order to allow applications to query them, resulting in significant\\nlatency penalties. This work presents Apollo: a low-latency mon-\\nitoring service that aims to provide applications and middleware\\nlibraries with direct access to relational telemetry data. Monitoring\\nthe system can create interference and overhead, slowing down raw\\nperformance of the resources for the job. However, having a current\\nview of the system can aid middleware services in making more\\noptimal decisions which can ultimately improve the overall perfor-\\nmance. Apollo has been designed from the ground up to provide\\nlow latency, using Publish\u2013Subscribe (Pub-Sub) semantics, and low\\noverhead, using adaptive intervals in order to change the length of\\ntime between polling the resource for telemetry data and machine\\nlearning in order to predict changes to the telemetry data between\\nactual resource polling. This work also provides some high level\\nabstractions called I/O curators, which can further aid middleware\\nlibraries and applications to make optimal decisions. Evaluations\\nshowcase that Apollo can achieve sub-millisecond latency for acquir-\\ning complex insights with a memory overhead of ~57MB and CPU\\noverhead being only 7% more than existing state-of-the-art systems.","authors":["N. Rajesh","H. Devarajan","J. Cernuda","K. Bateman","L. Logan","J. Ye","A. Kougkas","X.-H. Sun"],"date":"June, 2021","doi":"10.1145/3431379.3460640","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/rajesh2021apollo.bib","citation":"http://cs.iit.edu/~scs/assets/files/rajesh2021apollo.txt","pdf":"http://cs.iit.edu/~scs/assets/files/rajesh2021apollo.pdf"},"month":6,"slug":"rajesh-2021-apollo-dbd8","tags":["HPC","Machine Learning","Resource Monitoring","Hermes"],"title":"Apollo: An ML-assisted Real-Time Storage Resource Observer","type":"Conference","venue":"The 30th ACM International Symposium on High-Performance Parallel and Distributed Computing (HPDC\'21), June 21-25, 2021","year":2021},{"abstract":"Modern applications produce and process massive\\namounts of activity (or log) data. Traditional storage systems\\nwere not designed with an append-only data model and a\\nnew storage abstraction aims to fill this gap: the distributed\\nshared log store. However, existing solutions struggle to provide\\na scalable, parallel, and high-performance solution that can\\nsupport a diverse set of conflicting log workload requirements.\\nFinding the tail of a distributed log is a centralized point of\\ncontention. In this paper, we show how using physical time can\\nhelp alleviate the need of centralized synchronization points. We\\npresent ChronoLog, a new, distributed, shared, and multi-tiered\\nlog store that can handle more than a million tail operations\\nper second. Evaluation results show ChronoLog\'s potential,\\noutperforming existing solution by an order of magnitude.","authors":["A. Kougkas","H. Devarajan","K. Bateman","J. Cernuda","N. Rajesh","X.-H. Sun"],"date":"October, 2020","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/chronolog_temp.bib","citation":"http://cs.iit.edu/~scs/assets/files/chronolog_temp.txt","pdf":"http://cs.iit.edu/~scs/assets/files/kougkas2020chronolog.pdf"},"month":10,"slug":"kougkas-2020-chronolog-c458","tags":["Distributed Log","Shared Log","Tiered Storage","ChronoLog"],"title":"ChronoLog: A Distributed Shared Tiered Log Store with Time-based Data Ordering","type":"Conference","venue":"The 36th International Conference on Massive Storage Systems and Technology (MSST\'20), Oct. 29-30, 2020","year":2020},{"authors":["J. Cernuda","H. Trivino","H. Devarajan","A. Kougkas","X.-H. Sun"],"date":"November, 2019","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/cernuda2019eviction.bib","citation":"http://cs.iit.edu/~scs/assets/files/cernuda2019eviction.txt","extended abstract":"http://cs.iit.edu/~scs/assets/files/cernuda2019eviction_abstract.pdf","poster":"http://cs.iit.edu/~scs/assets/files/cernuda2019eviction.pdf"},"month":11,"slug":"cernuda-2019-efficient-data-b08d","tags":[],"title":"Efficient Data Eviction across Multiple Tiers of Storage","type":"Poster","venue":"The International Conference for High Performance Computing, Networking, Storage and Analysis (SC\'19)","year":2019}]')}}]);