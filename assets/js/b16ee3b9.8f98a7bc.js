"use strict";(self.webpackChunkgrc=self.webpackChunkgrc||[]).push([[68212],{77354:e=>{e.exports=JSON.parse('{"abstract":"Pre-training of LLMs and transformers is known to\\ntake weeks if not months even with powerful HPC systems. However,\\ninferences are an equally important problem: once pre-trained, the\\nmodel needs to serve a large number of inferences submitted under\\nconcurrency by multiple users. Thus, speeding up each inference\\nrequest is instrumental in achieving high throughput and latency at\\nscale. To avoid redundant recomputation in each decode iteration, a\\nKey-Value (KV) cache is used to store previously computed keys (K)\\nand values (V), speeding up token generation. GPU memory is\\nprimarily consumed by model weights and the remainder is used by\\nthe KV cache. Thus, the free GPU space available to the KV cache\\nis a scarce resource that needs to be managed in an efficient way in\\norder to minimize the overhead of redundant recomputations. There\\nare many optimizations applied in this context: batching of inference\\nrequests to enable them to run in the same forward pass (and thus\\nincrease the parallelism and inference throughput), different KV\\ncache eviction policies (simply drop KV entries and recompute them\\nlater vs. swap to host memory), etc. Under these circumstances, the\\ndecision of what batching strategy, what KV cache eviction policy\\nto apply and how the KV cache impacts the inference performance\\nis non-trivial. Unlike the case of pre-training, state-of-art studies\\nare scarce in this context. To fill this gap, in this paper we study\\nthe impact of KV caching. Specifically, we instrument vLLM to\\nmeasure and analyze fine-grain metrics (token throughput, KV\\ncache memory access patterns, load balancing of the forward passes),\\nduring different inference stages (prefill, decode) in several scenarios\\nthat involve concurrent inference requests using several benchmarks.\\nBased on the measurements and associated observations, we identify\\nseveral opportunities to improve the design of inference frameworks.\\nIndex Terms-LLM inference, KV cache profiling, access pattern\\ncharacterization.","authors":["J. Ye","J. Cernuda","A. Maurya","X.-H. Sun","A. Kougkas","B. Nicolae"],"date":"June, 2025","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/ye2025unboxkv_io.bib","citation":"http://cs.iit.edu/~scs/assets/files/ye2025unboxkv_io.txt","pdf":"http://cs.iit.edu/~scs/assets/files/ye2025unboxkv_io.pdf"},"month":6,"slug":"ye-2025-characterizing-behavior-f631","tags":["LABIOS"],"title":"Characterizing the Behavior and Impact of KV Caching on Transformer Inferences under Concurrency","type":"Conference","venue":"The 39th IEEE International Parallel & Distributed Processing Symposium (IPDPS 2025)","year":2025}')}}]);