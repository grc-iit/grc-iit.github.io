"use strict";(self.webpackChunkgrc=self.webpackChunkgrc||[]).push([[4516],{988:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>m,frontMatter:()=>l,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"type":"mdx","permalink":"/research/projects/viper","source":"@site/src/pages/research/projects/viper.mdx","title":"Viper: A High-Performance I/O Framework for Transferring Deep Neural Network Models","description":"Within a DL workflow, the scientific AI application and the inference serving system typically communicate","frontMatter":{"title":"Viper: A High-Performance I/O Framework for Transferring Deep Neural Network Models"},"unlisted":false}');var t=i(74848),s=i(28453),o=i(18845);const l={title:"Viper: A High-Performance I/O Framework for Transferring Deep Neural Network Models"},a="Viper: A High-Performance I/O Framework for Transferring Deep Neural Network Models",c={},d=[{value:"Background",id:"background",level:2},{value:"Motivation",id:"motivation",level:2},{value:"Approach",id:"approach",level:2},{value:"Evaluation Results",id:"evaluation-results",level:2},{value:"Members",id:"members",level:2}];function h(e){const n={h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)("p",{children:(0,t.jsx)("img",{src:i(24021).A,width:"200"})}),"\n",(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"viper-a-high-performance-io-framework-for-transferring-deep-neural-network-models",children:"Viper: A High-Performance I/O Framework for Transferring Deep Neural Network Models"})}),"\n",(0,t.jsx)(o.A,{projectId:"viper"}),"\n",(0,t.jsx)(n.p,{children:"Within a DL workflow, the scientific AI application and the inference serving system typically communicate\nthe DNN models through a model repository (e.g., PFS). However, this method may result in high model update\nlatency due to I/O bottlenecks of PFS and high model discovery latency due to the fixed-interval pull-based\nmodel detection mechanism. Moreover, both continuous learning and the scenario that consumer has a limited\ntime window for inferencing increases the model update frequency between producers and consumers. Model update\nfrequency affects both training and inference performance. Viper is a high-performance I/O framework aiming\nto accelerate the model exchange, and to find an optimal model update schedule to achieve high inference\nperformance while keeping low training cost."}),"\n",(0,t.jsx)(n.h2,{id:"background",children:"Background"}),"\n",(0,t.jsx)(n.p,{children:"In the traditional deep learning (DL) workflow:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Producer (Scientific AI Application) typically trains a DNN model offline with a fixed set of input data and then persists the trained model to a model repository for future use"}),"\n",(0,t.jsx)(n.li,{children:"Consumer (Inference Serving System) loads the pre-trained DNN model from the model repository and offers online inference queries for end-user applications"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"However, this offline training is not an ideal choice in two scenarios:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["**Scenario 1: ** Modern scientific DL workflows often operate in dynamic environments where new data is constantly changing and accumulating over time.","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"To adapt to data changes, continuous learning is utilized to continuously (re)-train a DNN model by using some online techniques."}),"\n",(0,t.jsx)(n.li,{children:"Continuous learning implies the continuous deployment of the DNN model to keep the model up-to-date"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["**Scenario 2: ** The consumer may have a limited time window for inferences, it may need to start inferencing after the warmup phase in model training on the producer side","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Producer continues training the model while the consumer conducts inferences"}),"\n",(0,t.jsx)(n.li,{children:"This requires the intermediate DNN models to be consistently delivered from the producer to the consumer during training to achieve high inference performance"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)("center",{children:(0,t.jsx)("p",{children:(0,t.jsx)("img",{src:i(66734).A,width:"600"})})}),"\n",(0,t.jsxs)(n.p,{children:["Both scenarios ",(0,t.jsx)(n.strong,{children:"increase the model update frequency"})," between producers and consumers."]}),"\n",(0,t.jsx)(n.h2,{id:"motivation",children:"Motivation"}),"\n",(0,t.jsx)("center",{children:(0,t.jsx)("p",{children:(0,t.jsx)("img",{src:i(34051).A,width:"600"})})}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["Model update frequency ",(0,t.jsx)(n.strong,{children:"affects both training and inference performance"}),", since a model update operation involves both model checkpointing and model data delivery.E.g.,","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Frequent model updates can enhance inference performance but may slow down training"}),"\n",(0,t.jsx)(n.li,{children:"Infrequent model updates may pose less overhead on training but may degrade the overall inference model accuracy"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["Currently, Scientific AI Applications and Inference Serving Systems communicate through a model repository (e.g., PFS), as depicted in Figure (a). This communication method may result in:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"High model update latency"})," due to the I/O bottlenecks caused by concurrent, uncoordinated, small I/O accesses to PFS"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"High model discovery High model discovery latency on consumers"})," due to the static fixed-interval pull-based (e.g., polling) detection mechanism"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Thus, there is a need to 1) ",(0,t.jsx)(n.strong,{children:"balance the trade-off"})," between training and inference performance; 2) ",(0,t.jsx)(n.strong,{children:"accelerate model data discovery and delivery"})," between producers and consumers (Figure b)."]}),"\n",(0,t.jsx)(n.h2,{id:"approach",children:"Approach"}),"\n",(0,t.jsx)("center",{children:(0,t.jsx)("p",{children:(0,t.jsx)("img",{src:i(63914).A,width:"600"})})}),"\n",(0,t.jsx)(n.p,{children:"Viper is a high-performance I/O framework to accelerate DNN models exchange between\nproducers and consumers. It aims to:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Balance the trade-off between training runtime and inference performance"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Viper builds an ",(0,t.jsx)(n.strong,{children:"intelligent inference performance predictor"})," to achieve this object","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Can ",(0,t.jsx)(n.strong,{children:"decide an optimal model checkpoint schedule"})," between producers and consumers"]}),"\n",(0,t.jsx)(n.li,{children:"Supporting two different algorithms for finding the optimal checkpoint schedule"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Accelerate model data transfer"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Viper creates a ",(0,t.jsx)(n.strong,{children:"novel cache-aware data transfer engine"})," to speedup model update between producers and consumers","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Creating a direct data exchange channel for model delivery and utilizes. E.g., the direct GPU-to-GPU or RAM-to-RAM data transfer strategy"}),"\n",(0,t.jsx)(n.li,{children:"Utilizing a lightweight ublish-subscribe notification mechanism to promptly inform the consumer of the model changes."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)("center",{children:(0,t.jsx)("p",{children:(0,t.jsx)("img",{src:i(2676).A,width:"400"})})}),"\n",(0,t.jsx)(n.h2,{id:"evaluation-results",children:"Evaluation Results"}),"\n",(0,t.jsxs)("center",{children:[(0,t.jsx)("h3",{children:"End-to-end Model Update Latency"}),(0,t.jsxs)("div",{style:{display:"flex",justifyContent:"start"},children:[(0,t.jsx)("div",{children:(0,t.jsx)("img",{src:i(5244).A,width:"300"})}),(0,t.jsx)("div",{style:{display:"flex",alignItems:"center",justifyContent:"center"},children:(0,t.jsxs)("ul",{style:{textAlign:"left"},children:[(0,t.jsx)("li",{children:"The Y-axis shows the end-to-end CANDLE-NT3 model update latency in seconds"}),(0,t.jsx)("li",{children:"Viper improves model update latency by"}),(0,t.jsxs)("ul",{style:{textAlign:"left"},children:[(0,t.jsxs)("li",{children:[(0,t.jsx)("strong",{children:"~10x"})," via GPU-to-GPU model transfer strategy"]}),(0,t.jsxs)("li",{children:[(0,t.jsx)("strong",{children:"~3.5x"})," via RAM-to-RAM model transfer strategy"]})]})]})})]}),(0,t.jsx)("h3",{children:"Benefits of Low-latency Model Update Strategy"}),(0,t.jsxs)("div",{style:{display:"flex",justifyContent:"space-between"},children:[(0,t.jsx)("div",{children:(0,t.jsx)("img",{src:i(60950).A,width:"400"})}),(0,t.jsx)("div",{style:{display:"flex",alignItems:"center",justifyContent:"center"},children:(0,t.jsxs)("ul",{style:{textAlign:"left"},children:[(0,t.jsx)("li",{children:"Application: CANDLE-NT3 model of 4.6GB size"}),(0,t.jsx)("li",{children:"The left Y-axis shows cumulative inference loss over 50000 inference requests"}),(0,t.jsx)("li",{children:"The right Y-axis shows training overhead added by model checkpointing"}),(0,t.jsx)("li",{children:"GPU-to-GPU and RAM-to-RAM model transfer strategies exhibit"}),(0,t.jsxs)("ul",{style:{textAlign:"left"},children:[(0,t.jsx)("li",{children:"lower cumulative inference loss"}),(0,t.jsx)("li",{children:"less training overhead time"})]})]})})]})]}),"\n",(0,t.jsx)(n.h2,{id:"members",children:"Members"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Jie Ye, Illinois Institute of Technology"}),"\n",(0,t.jsx)(n.li,{children:"Jaime Cernuda, Illinois Institute of Technology"}),"\n",(0,t.jsx)(n.li,{children:"Bogdan Nicolae, Argonne National Laboratory"}),"\n",(0,t.jsx)(n.li,{children:"Anthony Kougkas, Illinois Institute of Technology"}),"\n",(0,t.jsx)(n.li,{children:"Xian-He Sun, Illinois Institute of Technology"}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}},18845:(e,n,i)=>{i.d(n,{A:()=>d});i(96540);var r=i(34164),t=i(46784),s=i(66188),o=i(71429),l=i(66588);const a={badgeDarker:"badgeDarker_Lm_2"};var c=i(74848);function d({addMargin:e=!0,projectId:n}){const{projects:i}=(0,l.P_)("grc-plugin-projects"),d=(0,o.G)(i,n);if(!d)return null;const{isOurs:h=!1,sourceLink:m,tutorialLink:u,type:p}=d,g="funded"===p;return g||h||m||u?(0,c.jsxs)("div",{className:(0,r.A)(e&&"margin-bottom--md"),children:[h&&(0,c.jsx)("span",{className:"badge badge--primary margin-right--xs",children:"GRC-led"}),g&&(0,c.jsx)("span",{className:"badge badge--success margin-right--xs",children:"Funded"}),void 0!==m&&(0,c.jsxs)("a",{className:(0,r.A)("badge badge--secondary margin-right--xs",a.badgeDarker),href:m,rel:"noreferrer",style:{color:"var(--ifm-color-black) !important"},target:"_blank",children:["Open Source",(0,c.jsx)(t.g,{className:"margin-left--xs",icon:s.Ju_,size:"sm",style:{color:"var(--ifm-color-black)"}})]}),void 0!==u&&(0,c.jsxs)("a",{className:"badge badge--danger margin-right--xs",href:u,rel:"noreferrer",style:{backgroundColor:"var(--ifm-color-warning-lightest) !important",borderColor:"var(--ifm-color-warning-lightest) !important",color:"var(--ifm-color-black) !important"},target:"_blank",children:["Tutorial",(0,c.jsx)(t.g,{className:"margin-left--xs",icon:s.Ju_,size:"sm",style:{color:"var(--ifm-color-black)"}})]})]}):null}},71429:(e,n,i)=>{i.d(n,{G:()=>r});const r=(e,n)=>e.find((e=>e.id===n))},60950:(e,n,i)=>{i.d(n,{A:()=>r});const r=i.p+"assets/images/benefits_low_latency_strategy-f5f6fc2cb955c936c1e9b6fc8cbe4193.png"},2676:(e,n,i)=>{i.d(n,{A:()=>r});const r=i.p+"assets/images/data_transfer-bd621a0c8a42b3401593200c131b1c32.png"},5244:(e,n,i)=>{i.d(n,{A:()=>r});const r=i.p+"assets/images/end_to_end_latency-b64732777ddcba119a7224e8cc98fb17.png"},63914:(e,n,i)=>{i.d(n,{A:()=>r});const r=i.p+"assets/images/high_level_design-33c6a561fda3dd9c72e992691933f5dd.png"},24021:(e,n,i)=>{i.d(n,{A:()=>r});const r=i.p+"assets/images/logo-95bd4156d3a78ae2165cd6b0cd2d41b8.png"},66734:(e,n,i)=>{i.d(n,{A:()=>r});const r=i.p+"assets/images/scenario2-3f2e0ace5443aaec287e4fd1dd06be5c.png"},34051:(e,n,i)=>{i.d(n,{A:()=>r});const r=i.p+"assets/images/viper_motivation-59af44a57af0d841b49966ce02f4c5b6.png"},28453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>l});var r=i(96540);const t={},s=r.createContext(t);function o(e){const n=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);