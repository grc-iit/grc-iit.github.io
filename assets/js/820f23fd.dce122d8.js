"use strict";(self.webpackChunkgrc=self.webpackChunkgrc||[]).push([[83004],{78200:e=>{e.exports=JSON.parse('[{"abstract":"In the age of data-driven computing, integrating\\nHigh Performance Computing (HPC) and Big Data (BD) en-\\nvironments may be the key to increasing productivity and to\\ndriving scientific discovery forward. Scientific workflows consist\\nof diverse applications (i.e., HPC simulations and BD analysis)\\neach with distinct representations of data that introduce a se-\\nmantic barrier between the two environments. To solve scientific\\nproblems at scale, accessing semantically different data from\\ndifferent storage resources is the biggest unsolved challenge. In\\nthis work, we aim to address a critical question: \\"How can we\\nexploit the existing resources and efficiently provide transparent\\naccess to data from/to both environments\\". We propose iNtelligent\\nI/O Bridging Engine (NIOBE), a new data integration framework\\nthat enables integrated data access for scientific workflows with\\nasynchronous I/O and data aggregation. NIOBE performs the\\ndata integration using available I/O resources, in contrast to\\nexisting optimizations that ignore the I/O nodes present on the\\ndata path. In NIOBE, data access is optimized to consider both\\nthe ongoing production and the consumption of the data in\\nthe future. Experimental results show that with NIOBE, an\\nintegrated scientific workflow can be accelerated by up to 10x\\nwhen compared to a no-integration baseline and by up to 133%\\ncompared to other state-of-the-art integration solutions.","authors":["K. Feng","H. Devarajan","A. Kougkas","X.-H. Sun"],"date":"December, 2019","doi":"10.1109/bigdata47090.2019.9006363","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/kun2019NIOBE.bib","citation":"http://cs.iit.edu/~scs/assets/files/kun2019NIOBE.txt","pdf":"http://cs.iit.edu/~scs/assets/files/NIOBE_PDF_A.pdf"},"month":12,"slug":"feng-2019-niobe-19f9","tags":["Data Integration","Integrated Workflow","Data Aggregation","KVS","Parallel File System (PFS)"],"title":"NIOBE: An Intelligent I/O Bridging Engine for Complex and Distributed Workflows","type":"Conference","venue":"The 7th IEEE International Conference on Big Data, 2019. pp. 493-502","year":2019},{"abstract":"Modern High Performance Computing (HPC) ap-\\nplications, such as Earth science simulations, produce large\\namounts of data due to the surging of computing power, while\\nbig data applications have become more compute-intensive due\\nto increasingly sophisticated analysis algorithms. The needs of\\nboth HPC and big data technologies for advanced HPC and\\nbig data applications create a demand for integrated system\\nsupport. In this study, we introduce Scientific Data Processing\\n(SciDP) to support both HPC and big data applications via\\nintegrated scientific data processing. SciDP can directly process\\nscientific data stored on a Parallel File System (PFS), which\\nis typically deployed in an HPC environment, in a big data\\nprogramming environment running atop Hadoop Distributed File\\nSystem (HDFS). SciDP seamlessly integrates PFS, HDFS, and the\\nwidely-used R data analysis system to support highly efficient\\nprocessing of scientific data. It utilizes the merits of both PFS\\nand HDFS for fast data transfer, overlaps computing with\\ndata accessing, and integrates R into the data transfer process.\\nExperimental results show that SciDP accelerates analysis and\\nvisualization of a production NASA Center for Climate Simulation\\n(NCCS) climate and weather application by 6x to 8x when\\ncompared to existing solutions.","authors":["K. Feng","X.-H. Sun","X. Yang","S. Zhou"],"date":"September, 2018","doi":"10.1109/cluster.2018.00023","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/feng2018scidp.bib","citation":"http://cs.iit.edu/~scs/assets/files/feng2018scidp.txt","pdf":"http://cs.iit.edu/~scs/assets/files/SciDP_Kun2018.pdf"},"month":9,"slug":"feng-2018-scidp-cdc6","tags":[],"title":"SciDP: Support HPC and Big Data Applications via Integrated Scientific Data Processing","type":"Conference","venue":"The IEEE International Conference on Cluster Computing 2018 (Cluster\'18), Belfast, UK2018. pp. 114-123.","year":2018},{"abstract":"Data analytics becomes increasingly important in\\nbig data applications. Adaptively subsetting large amounts of\\ndata to extract the interesting events such as the centers of\\nhurricane or thunderstorm, statistically analyzing and visualizing\\nthe subset data, is an effective way to analyze ever-growing data.\\nThis is particularly crucial for analyzing Earth Science data,\\nsuch as extreme weather. The Hadoop ecosystem (i.e., HDFS,\\nMapReduce, Hive) provides a cost-efficient big data management\\nenvironment and is being explored for analyzing big Earth\\nScience data.\\nOur study investigates the potential of a MapReduce-like\\nparadigm to perform statistical calculations, and utilizes the\\ncalculated results to subset as well as visualize data in a scalable\\nand efficient way. RHadoop and SparkR are deployed to enable\\nR to access and process data in parallel with Hadoop and\\nSpark, respectively. The regular R libraries and tools are utilized\\nto create and manipulate images. Statistical calculations, such\\nas maximum and average variable values, are carried with R\\nor SQL. We have developed a strategy to conduct query and\\nvisualization within one phase, and thus significantly improve the\\noverall performance in a scalable way. The technical challenges\\nand limitations of both Hadoop and Spark platforms for R are\\nalso discussed.","authors":["X. Yang","S. Liu","K. Feng","S. Zhou","X.-H. Sun"],"date":"October, 2016","doi":"10.1109/bdcloud-socialcom-sustaincom.2016.24","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/yang2016visualization.bib","citation":"http://cs.iit.edu/~scs/assets/files/yang2016visualization.txt","pdf":"http://cs.iit.edu/~scs/assets/files/bdcloud16.pdf"},"month":10,"slug":"yang-2016-visualization-adaptive-4cd5","tags":[],"title":"Visualization and Adaptive Subsetting of Earth Science Data in HDFS - A Novel Data Analysis Strategy with Hadoop and Spark","type":"Conference","venue":"6th IEEE International Conference on Big Data and Cloud Computing (BDCloud 2016), Atlanta, GA2016, pp. 89-96","year":2016},{"authors":["H. Eslami","A. Kougkas","M. Kotsifakou","T. Kasampalis","K. Feng","Y. Lu","W. D. Gropp","X.-H. Sun","Y. Chen","R. Thakur"],"date":"November, 2015","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/eslami2015efficient.bib","citation":"http://cs.iit.edu/~scs/assets/files/eslami2015efficient.txt","pdf":"https://dl.acm.org/ft_gateway.cfm?ftid=1643727&id=2831249"},"month":11,"slug":"eslami-2015-efficient-disk-fda9","tags":[],"title":"Efficient Disk-to-Disk Sorting: A Case Study in Decoupled Execution Paradigm","type":"Workshop","venue":"Data Intensive Scalable Computing Systems Workshop (DISCS), in conjunction with ACM/IEEE SuperComputing 2015, Austin, TX, USA","year":2015},{"abstract":"The collective communication operations, which\\nare widely used in parallel applications for global commu-\\nnication and synchronization are critical for application\'s\\nperformance and scalability. However, how faulty collective\\ncommunications impact the application and how errors prop-\\nagate between the application processes is largely unexplored.\\nOne of the critical reasons for this situation is the lack of fast\\nevaluation method to investigate the impacts of faulty collective\\noperations. The traditional random fault injection methods\\nrelying on a large amount of fault injection tests to ensure\\nstatistical significance require a significant amount of resources\\nand time. These methods result in prohibitive evaluation cost\\nwhen applied to the collectives.\\nIn this paper, we introduce a novel tool named Fast Fault\\nInjection and Sensitivity Analysis Tool (FastFIT) to conduct\\nfast fault injection and characterize the application sensitivity\\nto faulty collectives. The tool achieves fast exploration by\\nreducing the exploration space and predicting the application\\nsensitivity using Machine Learning (ML) techniques. A basis\\nfor these techniques are implicit correlations between MPI\\nsemantics, application context, critical application features,\\nand application responses to faulty collective communications.\\nThe experimental results show that our approach reduces\\nthe fault injection points and tests by 97% for represen-\\ntative benchmarks (NAS Parallel Benchmarks (NPB)) and a\\nrealistic application (Large-scale Atomic/Molecular Massively\\nParallel Simulator (LAMMPS)) on a production supercomputer.\\nFurther, we statistically generalize the application sensitivity\\nto faulty collective communications for these workloads, and\\npresent correlation between application features and the sen-\\nsitivity.","authors":["K. Feng","M. G. Venkata","D. Li","X.-H. Sun"],"date":"September, 2015","doi":"10.1109/cluster.2015.31","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/feng2015fast.bib","citation":"http://cs.iit.edu/~scs/assets/files/feng2015fast.txt","pdf":"http://cs.iit.edu/~scs/assets/files/fastfit.pdf"},"month":9,"slug":"feng-2015-fast-fault-d4c7","tags":[],"title":"Fast Fault Injection and Sensitivity Analysis for Collective Communications","type":"Conference","venue":"IEEE International Conference on Cluster Computing 2015 (Cluster\'15), Chicago, IL, USA","year":2015},{"abstract":"Hadoop, as one of the most widely accepted MapReduce frameworks, is naturally data-intensive. Its several dependent projects, such as Mahout and Hive, inherent this characteristic. Meanwhile I/O optimization becomes a daunting work, since applications\' source code is not always available. I/O traces for Hadoop and its dependents are increasingly important, because it can faithfully reveal intrinsic I/O behaviors without knowing the source code. This method can not only help to diagnose system bottlenecks but also further optimize performance. To achieve this goal, we propose a transparent tracing and analysis tool suite, namely IOSIG+, which can be plugged into Hadoop system. We make several contributions: 1) we describe our approach of tracing; 2) we release the tracer, which can trace I/O operations without modifying targets\' source code; 3) this work adopts several techniques to mitigate the introduced execution overhead at runtime; 4) we create an analyzer, which helps to discover new approaches to address I/O problems according to access patterns. The experimental results and analysis confirm its effectiveness and the observed overhead can be as low as 1.97%.","authors":["B. Feng","X. Yang","K. Feng","Y. Yin","X.-H. Sun"],"date":"September, 2015","doi":"10.1109/cluster.2015.17","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/feng2015iosig+.bib","citation":"http://cs.iit.edu/~scs/assets/files/feng2015iosig+.txt","pdf":"http://cs.iit.edu/~scs/assets/files/bo_cluster15_IOSIG+.pdf"},"month":9,"slug":"feng-2015-iosig--5e1a","tags":[],"title":"IOSIG+: on the Role of I/O Tracing and Analysis for Hadoop Systems","type":"Workshop","venue":"IEEE International Conference on Cluster Computing 2015 (Cluster\'15), Chicago, IL, USA","year":2015},{"abstract":"Key-Value Stores (KVStore) are being widely used\\nas the storage system for large-scale Internet services and cloud\\nstorage systems. However, they are rarely used in HPC systems,\\nwhere parallel file systems (PFS) are the dominant storage\\nsystems. In this study, we carefully examine the architecture\\ndifference and performance characteristics of PFS and KVStore.\\nWe propose that it is valuable to utilize KVStore to optimize\\nthe overall I/O performance, especially for the workloads that\\nPFS cannot handle well, such as the cases with hurtful data\\nsynchronization or heavy metadata operations. To verify this\\nproposal, we conducted comprehensive experiments with several\\nsynthetic benchmarks, an I/O benchmark, and a real application.\\nThe results show that our proposal is promising.","authors":["Y. Yin","A. Kougkas","K. Feng","H. Eslami","Y. Lu","X.-H. Sun","R. Thakur","W. D. Gropp"],"date":"November, 2014","doi":"10.1109/discs.2014.11","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/yin2014rethinking.bib","citation":"http://cs.iit.edu/~scs/assets/files/yin2014rethinking.txt","pdf":"http://cs.iit.edu/~scs/assets/files/KVStore-DISCS-2014.pdf"},"month":11,"slug":"yin-2014-rethinking-key-484a","tags":[],"title":"Rethinking Key-Value Store for Parallel I/O Optimization","type":"Workshop","venue":"Data Intensive Scalable Computing Systems Workshop (DISCS), in conjunction with ACM/IEEE SuperComputing 2014, New Orleans, LA, USA","year":2014},{"authors":["C. Chen","Y. Chen","K. Feng","Y. Yin","H. Eslami","R. Thakur","X.-H. Sun","W. D. Gropp"],"date":"September, 2014","doi":"10.1109/icppw.2014.48","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/chen2014decoupled.bib","citation":"http://cs.iit.edu/~scs/assets/files/chen2014decoupled.txt","pdf":"https://ieeexplore.ieee.org/iel7/7101393/7103416/07103466.pdf"},"month":9,"slug":"chen-2014-decoupled-io-8543","tags":[],"title":"Decoupled I/O for Data-Intensive High Performance Computing","type":"Workshop","venue":"Seventh International Workshop on Parallel Programming Models and Systems Software for High-End Computing (P2S2), in conjunction with the International Conference on Parallel Processing (ICPP-2014), Minneapolis, MN, USA","year":2014},{"abstract":"Hybrid parallel file systems (PFS), which consist of both\\nHDD and SSD servers, provide a promising solution for data-intensive\\napplications. In this study, we propose a performance-aware data place-\\nment (PADP) strategy to enable efficient data layout in hybrid PFSs.\\nThe basic idea of PADP is to dispatch data on different file servers with\\nadaptive varied-size file stripes based on the server storage performance.\\nBy using an effective data access cost model and a linear programming\\noptimization method, the appropriate stripe sizes for each file server are\\ndetermined effectively. We have implemented PADP within OrangeFS,\\na widely used parallel file system in HPC domain. Experimental results\\nof representative benchmark show that PADP can significantly improve\\nthe I/O performance of hybrid PFSs.","authors":["S. He","X.-H. Sun","B. Feng","K. Feng"],"date":"August, 2014","doi":"10.1007/978-3-319-11197-1_43","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/he2014performance.bib","citation":"http://cs.iit.edu/~scs/assets/files/he2014performance.txt","pdf":"http://cs.iit.edu/~scs/assets/files/padp.pdf"},"month":8,"slug":"he-2014-performance-aware-5536","tags":[],"title":"Performance-Aware Data Placement in Hybrid Parallel File Systems","type":"Conference","venue":"14th International Conference on Algorithms and Architectures for Parallel Processing (ICA3PP), Dalian, China","year":2014},{"authors":["K. Feng","Y. Yin","C. Chen","H. Eslami","X.-H. Sun","Y. Chen","R. Thakur","W. D. Gropp"],"date":"September, 2013","doi":"10.1109/cluster.2013.6702642","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/feng2013runtime.bib","citation":"http://cs.iit.edu/~scs/assets/files/feng2013runtime.txt","pdf":"https://www.computer.org/csdl/proceedings/cluster/2013/9999/00/06702642.pdf"},"month":9,"slug":"feng-2013-runtime-system-77fc","tags":[],"title":"Runtime System Design of Decoupled Execution Paradigm for Data-Intensive High-End Computing (Poster Presentation)","type":"Conference","venue":"IEEE International Conference on Cluster Computing 2013 (Cluster\'13), Indianapolis, IN, USA","year":2013},{"abstract":"Parallel I/O systems represent the most commonly\\nused engineering solution to mitigate the performance mismatch\\nbetween CPU and disk performance; however, parallel I/O sys-\\ntems are application dependent and may not work well for certain\\ndata access requests. New emerging solid state drives (SSD) are\\nable to deliver better performance but incur a high monetary\\ncost. While SSDs cannot always replace HDDs, the hybrid SSD-\\nHDD approach uniquely addresses common performance issues\\nin parallel I/O systems. The performance of hybrid SSD-HDD\\narchitecture depends on the utilization of the SSD and scheduling\\nof data placement. In this paper, we propose a cost-aware region-\\nlevel (CARL) data placement scheme for hybrid parallel I/O\\nsystems. CARL divides large files into several small regions,\\ncalculates the region costs according to the data access patterns,\\nand selectively places regions with high access costs onto the SSD-\\nbased file servers. We have implemented CARL under MPI-IO\\nand the PVFS2 parallel file system environment. Experimental\\nresults of representative benchmarks show that CARL is both\\nfeasible and able to improve I/O performance significantly.","authors":["S. He","X.-H. Sun","B. Feng","X. Huang","K. Feng"],"date":"September, 2013","doi":"10.1109/cluster.2013.6702615","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/he2013cost.bib","citation":"http://cs.iit.edu/~scs/assets/files/he2013cost.txt","pdf":"http://cs.iit.edu/~scs/assets/files/shuibing_cluster13.pdf"},"month":9,"slug":"he-2013-cost-aware-c52e","tags":[],"title":"A Cost-Aware Region-Level Data Placement Scheme for Hybrid Parallel I/O Systems","type":"Conference","venue":"IEEE International Conference on Cluster Computing 2013 (Cluster\'13), Indianapolis, IN, USA","year":2013}]')}}]);