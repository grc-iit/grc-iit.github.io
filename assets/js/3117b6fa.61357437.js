"use strict";(self.webpackChunkgrc=self.webpackChunkgrc||[]).push([[4516],{988:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>c,default:()=>p,frontMatter:()=>o,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"type":"mdx","permalink":"/research/projects/viper","source":"@site/src/pages/research/projects/viper.mdx","title":"Viper: A High-Performance I/O Framework for Transparently Updating, Storing, and Transferring Deep Neural Network Models","description":"Overview","frontMatter":{"title":"Viper: A High-Performance I/O Framework for Transparently Updating, Storing, and Transferring Deep Neural Network Models"},"unlisted":false}');var t=r(74848),s=r(28453),a=r(18845);const o={title:"Viper: A High-Performance I/O Framework for Transparently Updating, Storing, and Transferring Deep Neural Network Models"},c="Viper: A High-Performance I/O Framework for Transparently Updating, Storing, and Transferring Deep Neural Network Models",l={},d=[{value:"Overview",id:"overview",level:2},{value:"Key Contributions",id:"key-contributions",level:2},{value:"Background",id:"background",level:2},{value:"Motivation",id:"motivation",level:2},{value:"Approach",id:"approach",level:2},{value:"Viper&#39;s High-level Architecture",id:"vipers-high-level-architecture",level:3},{value:"Accelerate Model Data Transfer",id:"accelerate-model-data-transfer",level:3},{value:"Experiment Results",id:"experiment-results",level:2},{value:"End-to-end Model Update Latency",id:"end-to-end-model-update-latency",level:3},{value:"Inference Performance Predictor",id:"inference-performance-predictor",level:3},{value:"Members",id:"members",level:2}];function h(e){const n={h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)("p",{children:(0,t.jsx)("img",{src:r(24021).A,width:"200"})}),"\n",(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"viper-a-high-performance-io-framework-for-transparently-updating-storing-and-transferring-deep-neural-network-models",children:"Viper: A High-Performance I/O Framework for Transparently Updating, Storing, and Transferring Deep Neural Network Models"})}),"\n",(0,t.jsx)(a.A,{projectId:"viper"}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"Scientific workflows are increasingly using Deep Learning (DL), requiring a Deep Neural Network (DNN) model to be\ntrained and used for inferences at the same time. A common approach is for the training server (producer) and the\ninference server (consumer) to use separate model replicas that are kept synchronized. This setup, however, creates\ntwo major challenges:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Trade-off between Updates and Performance"}),": A frequent model update schedule can improve inference quality because\nthe consumer uses a more up-to-date model, but it can also slow down the training process due to the overhead of\ncreating and transferring checkpoints. Conversely, infrequent updates may lead to less precise inference results."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Inefficient Model Transfer"}),": Traditional methods for sharing models between the producer and consumer often rely on\nan intermediate staging area (e.g., PFS), causing significant delays due to I/O bottlenecks and the use of fixed-interval\npolling by the consumer to detect new models."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"key-contributions",children:"Key Contributions"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Viper"})," is high-performance I/O framework designed to both determine a near-optimal checkpoint schedule and accelerate the delivery\nof model updates. It is built on two core innovations:"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Inference Performance Predictor (IPP)"}),": Identifies a near-optimal checkpoint schedule to effectively\nbalance the trade-off between training slowdown and inference quality improvement."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Memory-first Model Transfer Engine"}),": Accelerates model delivery by using direct\nmemory-to-memory communication, bypassing slower storage like PFS. It prioritizes GPU-to-GPU memory transfer when\navailable, falling back to host-to-host RDMA transfer if needed. This asynchronous engine, combined with a push-based\nnotification module, ensures that consumers are promptly notified of new model updates without relying on polling."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"background",children:"Background"}),"\n",(0,t.jsx)(n.p,{children:"In traditional DL workflow, producer (Scientific AI Application) typically trains a DNN model offline with a fixed set of input data\nand then persists the trained model to a model repository for future use, while consumer (Inference Serving System) will load the pre-trained\nDNN model from the model repository and offers online inference queries for end-user applications."}),"\n",(0,t.jsx)(n.p,{children:"However, this offline training is not an ideal choice in two scenarios:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Scenario 1:"})," Modern scientific DL workflows often operate in dynamic environments where new data is constantly changing and accumulating over time.","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"To adapt to data changes, continuous learning is utilized to continuously (re)-train a DNN model by using some online techniques."}),"\n",(0,t.jsx)(n.li,{children:"Continuous learning implies the continuous deployment of the DNN model to keep the model up-to-date"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Scenario 2:"})," The consumer may have a limited time window for inferences, it may need to start inferencing after the warmup phase in model training on the producer side","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Producer continues training the model while the consumer conducts inferences"}),"\n",(0,t.jsx)(n.li,{children:"This requires the intermediate DNN models to be consistently delivered from the producer to the consumer during training to achieve high inference performance"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)("center",{children:(0,t.jsx)("p",{children:(0,t.jsx)("img",{src:r(66734).A,width:"600"})})}),"\n",(0,t.jsxs)(n.p,{children:["Both scenarios ",(0,t.jsx)(n.strong,{children:"increase the model update frequency"})," between producers and consumers."]}),"\n",(0,t.jsx)(n.h2,{id:"motivation",children:"Motivation"}),"\n",(0,t.jsx)("center",{children:(0,t.jsx)("p",{children:(0,t.jsx)("img",{src:r(34051).A,width:"600"})})}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["Model update frequency ",(0,t.jsx)(n.strong,{children:"affects both training and inference performance"}),", since a model update operation involves both model checkpointing and model data delivery.E.g.,","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Frequent model updates can enhance inference performance but may slow down training"}),"\n",(0,t.jsx)(n.li,{children:"Infrequent model updates may pose less overhead on training but may degrade the overall inference model accuracy"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["Currently, Scientific AI Applications and Inference Serving Systems communicate through a model repository (e.g., PFS), as depicted in Figure (a). This communication method may result in:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"High model update latency"})," due to the I/O bottlenecks caused by concurrent, uncoordinated, small I/O accesses to PFS"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"High model discovery High model discovery latency on consumers"})," due to the static fixed-interval pull-based (e.g., polling) detection mechanism"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Thus, there is a need to 1) ",(0,t.jsx)(n.strong,{children:"balance the trade-off"})," between training and inference performance; 2) ",(0,t.jsx)(n.strong,{children:"accelerate model data discovery and delivery"})," between producers and consumers (Figure b)."]}),"\n",(0,t.jsx)(n.h2,{id:"approach",children:"Approach"}),"\n",(0,t.jsx)(n.h3,{id:"vipers-high-level-architecture",children:"Viper's High-level Architecture"}),"\n",(0,t.jsx)("center",{children:(0,t.jsx)("p",{children:(0,t.jsx)("img",{src:r(63914).A,width:"600"})})}),"\n",(0,t.jsx)(n.p,{children:"Viper is a high-performance I/O framework to accelerate DNN models exchange between\nproducers and consumers. It aims to:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Balance the trade-off between training runtime and inference performance"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Viper builds an ",(0,t.jsx)(n.strong,{children:"intelligent inference performance predictor"})," to achieve this object","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Can ",(0,t.jsx)(n.strong,{children:"decide an optimal model checkpoint schedule"})," between producers and consumers"]}),"\n",(0,t.jsx)(n.li,{children:"Supporting two different algorithms for finding the optimal checkpoint schedule"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Accelerate model data transfer"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Viper creates a ",(0,t.jsx)(n.strong,{children:"novel cache-aware data transfer engine"})," to speedup model update between producers and consumers","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Creating a direct data exchange channel for model delivery and utilizes. E.g., the direct GPU-to-GPU or RAM-to-RAM data transfer strategy"}),"\n",(0,t.jsx)(n.li,{children:"Utilizing a lightweight publish-subscribe notification mechanism to promptly inform the consumer of the model changes."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"accelerate-model-data-transfer",children:"Accelerate Model Data Transfer"}),"\n",(0,t.jsx)("center",{children:(0,t.jsx)("p",{children:(0,t.jsx)("img",{src:r(2676).A,width:"400"})})}),"\n",(0,t.jsx)(n.p,{children:"During training, DNN models can be cached on multiple alternative locations (e.g., GPU memory, Host memory, and PFS)"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["Asynchronous memory-first engine","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Utilize the cached models on different locations to accelerate data movement between producer and consumer"}),"\n",(0,t.jsxs)(n.li,{children:["Creates a direct communication channel to transfer the model data","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Direct GPU-to-GPU memory and host-to-host memory data transfer strategy"}),"\n",(0,t.jsx)(n.li,{children:"Implemented based on MPI library"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"A lightweight publish-subscribe notification module"})," to proactively inform consumers of model updates instead\nof passively periodic queries"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"experiment-results",children:"Experiment Results"}),"\n",(0,t.jsx)(n.h3,{id:"end-to-end-model-update-latency",children:"End-to-end Model Update Latency"}),"\n",(0,t.jsx)("center",{children:(0,t.jsx)("div",{style:{display:"flex",flexDirection:"column",alignItems:"flex-start",gap:"12px"},children:(0,t.jsxs)("div",{style:{display:"flex",gap:"16px"},children:[(0,t.jsx)("img",{src:r(73215).A,width:"250"}),(0,t.jsx)("img",{src:r(17808).A,width:"250"}),(0,t.jsx)("img",{src:r(13426).A,width:"250"})]})})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Goal:"})," showcase the end-to-end model update latency across different model transfer strategies"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Observations:"})}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["Both GPU-to-GPU and Host-to-Host memory strategies achieve better performance than PFS","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"GPU-to-GPU outperforms the baseline by 12x for NT3, 9x for TC1, and 15x for PtychoNN using Viper-Async approach"}),"\n",(0,t.jsx)(n.li,{children:"Host-to-Host using Viper-Async approach is at least 3x better than baseline"}),"\n",(0,t.jsx)(n.li,{children:"This is attributed to high I/O bandwidth of the fast memory tiers and the high-speed network"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.li,{children:"Viper-PFS approach is also ~1.2x faster than baseline since it only writes model weights and closely related metadata into the file"}),"\n",(0,t.jsx)(n.li,{children:"Viper-Async is slower than Viper-sync because it uses a separate thread for data transfer to reduce training interruption, requiring an extra data copy"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"inference-performance-predictor",children:"Inference Performance Predictor"}),"\n",(0,t.jsx)("center",{children:(0,t.jsx)("div",{style:{display:"flex",justifyContent:"space-between"},children:(0,t.jsxs)("div",{style:{display:"flex",gap:"16px"},children:[(0,t.jsx)("img",{src:r(71530).A,width:"250"}),(0,t.jsx)("img",{src:r(53031).A,width:"250"}),(0,t.jsx)("img",{src:r(18952).A,width:"250"})]})})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Goal:"})," showcase the checkpoint schedule identified by the Inference Performance Predictor (IPP) can achieve lower CIL compared to baseline"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Observations:"})}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Both fixed-interval and adaptive-interval checkpoint schedule can achieve better CIL compared with the baseline (i.e., epoch-boundary checkpoint schedule)"}),"\n",(0,t.jsxs)(n.li,{children:["Adaptive-interval checkpoint schedule is better than fixed-interval approach","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"For NT3 model, adaptive-interval schedule reduces the CIL from 3.8k to 3.0k"}),"\n",(0,t.jsx)(n.li,{children:"For TC1 model,  adaptive-interval schedule reduces the CIL from the 32.8k to 30.4k"}),"\n",(0,t.jsx)(n.li,{children:"For PtychoNN model, adaptive-interval schedule reduce the CIL from the baseline of 66.2k to 4.0k"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"members",children:"Members"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Jie Ye, Illinois Institute of Technology"}),"\n",(0,t.jsx)(n.li,{children:"Jaime Cernuda, Illinois Institute of Technology"}),"\n",(0,t.jsx)(n.li,{children:"Bogdan Nicolae, Argonne National Laboratory"}),"\n",(0,t.jsx)(n.li,{children:"Anthony Kougkas, Illinois Institute of Technology"}),"\n",(0,t.jsx)(n.li,{children:"Xian-He Sun, Illinois Institute of Technology"}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}},18845:(e,n,r)=>{r.d(n,{A:()=>d});r(96540);var i=r(34164),t=r(46784),s=r(66188),a=r(71429),o=r(66588);const c={badgeDarker:"badgeDarker_Lm_2"};var l=r(74848);function d({addMargin:e=!0,projectId:n}){const{projects:r}=(0,o.P_)("grc-plugin-projects"),d=(0,a.G)(r,n);if(!d)return null;const{isOurs:h=!1,sourceLink:p,tutorialLink:m,type:u}=d,g="funded"===u;return g||h||p||m?(0,l.jsxs)("div",{className:(0,i.A)(e&&"margin-bottom--md"),children:[h&&(0,l.jsx)("span",{className:"badge badge--primary margin-right--xs",children:"GRC-led"}),g&&(0,l.jsx)("span",{className:"badge badge--success margin-right--xs",children:"Funded"}),void 0!==p&&(0,l.jsxs)("a",{className:(0,i.A)("badge badge--secondary margin-right--xs",c.badgeDarker),href:p,rel:"noreferrer",style:{color:"var(--ifm-color-black) !important"},target:"_blank",children:["Open Source",(0,l.jsx)(t.g,{className:"margin-left--xs",icon:s.Ju_,size:"sm",style:{color:"var(--ifm-color-black)"}})]}),void 0!==m&&(0,l.jsxs)("a",{className:"badge badge--danger margin-right--xs",href:m,rel:"noreferrer",style:{backgroundColor:"var(--ifm-color-warning-lightest) !important",borderColor:"var(--ifm-color-warning-lightest) !important",color:"var(--ifm-color-black) !important"},target:"_blank",children:["Tutorial",(0,l.jsx)(t.g,{className:"margin-left--xs",icon:s.Ju_,size:"sm",style:{color:"var(--ifm-color-black)"}})]})]}):null}},71429:(e,n,r)=>{r.d(n,{G:()=>i});const i=(e,n)=>e.find((e=>e.id===n))},2676:(e,n,r)=>{r.d(n,{A:()=>i});const i=r.p+"assets/images/data_transfer-bd621a0c8a42b3401593200c131b1c32.png"},63914:(e,n,r)=>{r.d(n,{A:()=>i});const i=r.p+"assets/images/high_level_design-fa5f7f1ffcecd002a2449cbe17b93857.png"},73215:(e,n,r)=>{r.d(n,{A:()=>i});const i=r.p+"assets/images/latency_nt3_600-37ecde0b110ba468f4ef947a77e9e49a.png"},13426:(e,n,r)=>{r.d(n,{A:()=>i});const i=r.p+"assets/images/latency_ptychoNN-1a7c723c9e8a4cd6f1819aa8e36225fd.png"},17808:(e,n,r)=>{r.d(n,{A:()=>i});const i=r.p+"assets/images/latency_tc1_4600-8d0db1038c9b1e91ac4b6143ac8eb0d6.png"},24021:(e,n,r)=>{r.d(n,{A:()=>i});const i=r.p+"assets/images/logo-95bd4156d3a78ae2165cd6b0cd2d41b8.png"},18952:(e,n,r)=>{r.d(n,{A:()=>i});const i=r.p+"assets/images/predictor_PtychoNN-8312de2f0659d6cc11de7ce312c370d9.png"},53031:(e,n,r)=>{r.d(n,{A:()=>i});const i=r.p+"assets/images/predictor_TC1-b192a364e1198a3f787a9ab59492c000.png"},71530:(e,n,r)=>{r.d(n,{A:()=>i});const i=r.p+"assets/images/predictor_nt3-14d3b0ea2b416cecde04ddd25ac44fe4.png"},66734:(e,n,r)=>{r.d(n,{A:()=>i});const i=r.p+"assets/images/scenario2-3f2e0ace5443aaec287e4fd1dd06be5c.png"},34051:(e,n,r)=>{r.d(n,{A:()=>i});const i=r.p+"assets/images/viper_motivation-adb60fce2603c4cdc64d2c5c67b6f5fc.png"},28453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>o});var i=r(96540);const t={},s=i.createContext(t);function a(e){const n=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);