"use strict";(self.webpackChunkgrc=self.webpackChunkgrc||[]).push([[39483],{5636:e=>{e.exports=JSON.parse('{"abstract":"Graphic Processing Units (GPUs) have limited mem-\\nory capacity. Training popular deep neural networks (DNNs)\\noften requires a larger amount of memory than that a GPU may\\nhave. Consequently, training data needs to be swapped between\\nCPUs and GPUs. Data swapping may become a bottleneck when\\nits latency is longer than the latency of DNN computations.\\nTensor compression in GPUs can reduce the data swapping\\ntime. However, existing works on compressing tensors in the\\nvirtual memory of GPUs have two major issues: sub-optimal\\ncompression performance for varying tensor sparsity and sizes\\nand lack of portability because its implementation requires\\nadditional (de)compression units in memory controllers.\\nWe propose a self-tuning tensor compression framework,\\nnamed CSWAP, for improving the virtual memory management\\nof GPUs. It has high portability and is minimally dependent on\\nGPU architecture features. Furthermore, its runtime only applies\\ncompression on tensors that are deemed to be cost-effective\\nconsidering their sparsity and size and the characteristics of com-\\npression algorithms. Finally, our framework is fully automated\\nand can customize the compression policy for different neural\\nnetwork architectures and GPU architectures. Our experimental\\nresults using six representative memory-intensive DNN models\\nshow that CSWAP reduces tensor swapping latency by up to\\n50.9% and reduces the DNN training time by 20.7% on average\\nwith NVIDIA V100 GPUs compared to vDNN.\\nIndex Terms-DNN, GPU, Tensor, Swapping, Data Compres-\\nsion","authors":["P. Chen","S. He","X. Zhang","S. Chen","P. Hong","Y. Yin","X.-H. Sun","G. Chen"],"date":"September, 2021","doi":"10.1109/cluster48925.2021.00019","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/chen2021CSWAP.bib","citation":"http://cs.iit.edu/~scs/assets/files/chen2021CSWAP.txt","pdf":"http://cs.iit.edu/~scs/assets/files/chen2021CSWAP.pdf"},"month":9,"slug":"chen-2021-cswap-221d","tags":[],"title":"CSWAP: A Self-Tuning Compression Framework for Accelerating Tensor Swapping in GPUs","type":"Conference","venue":"The 2021 IEEE International Conference on Cluster Computing (CLUSTER\'21), September 7-10, 2021","year":2021}')}}]);