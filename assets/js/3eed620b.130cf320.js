"use strict";(self.webpackChunkgrc=self.webpackChunkgrc||[]).push([[75726],{2898:e=>{e.exports=JSON.parse('[{"abstract":"Pre-training of LLMs and transformers is known to\\ntake weeks if not months even with powerful HPC systems. However,\\ninferences are an equally important problem: once pre-trained, the\\nmodel needs to serve a large number of inferences submitted under\\nconcurrency by multiple users. Thus, speeding up each inference\\nrequest is instrumental in achieving high throughput and latency at\\nscale. To avoid redundant recomputation in each decode iteration, a\\nKey-Value (KV) cache is used to store previously computed keys (K)\\nand values (V), speeding up token generation. GPU memory is\\nprimarily consumed by model weights and the remainder is used by\\nthe KV cache. Thus, the free GPU space available to the KV cache\\nis a scarce resource that needs to be managed in an efficient way in\\norder to minimize the overhead of redundant recomputations. There\\nare many optimizations applied in this context: batching of inference\\nrequests to enable them to run in the same forward pass (and thus\\nincrease the parallelism and inference throughput), different KV\\ncache eviction policies (simply drop KV entries and recompute them\\nlater vs. swap to host memory), etc. Under these circumstances, the\\ndecision of what batching strategy, what KV cache eviction policy\\nto apply and how the KV cache impacts the inference performance\\nis non-trivial. Unlike the case of pre-training, state-of-art studies\\nare scarce in this context. To fill this gap, in this paper we study\\nthe impact of KV caching. Specifically, we instrument vLLM to\\nmeasure and analyze fine-grain metrics (token throughput, KV\\ncache memory access patterns, load balancing of the forward passes),\\nduring different inference stages (prefill, decode) in several scenarios\\nthat involve concurrent inference requests using several benchmarks.\\nBased on the measurements and associated observations, we identify\\nseveral opportunities to improve the design of inference frameworks.\\nIndex Terms-LLM inference, KV cache profiling, access pattern\\ncharacterization.","authors":["J. Ye","J. Cernuda","A. Maurya","X.-H. Sun","A. Kougkas","B. Nicolae"],"date":"June, 2025","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/ye2025unboxkv_io.bib","citation":"http://cs.iit.edu/~scs/assets/files/ye2025unboxkv_io.txt","pdf":"http://cs.iit.edu/~scs/assets/files/ye2025unboxkv_io.pdf"},"month":6,"slug":"ye-2025-characterizing-behavior-f631","tags":["LABIOS"],"title":"Characterizing the Behavior and Impact of KV Caching on Transformer Inferences under Concurrency","type":"Conference","venue":"The 39th IEEE International Parallel & Distributed Processing Symposium (IPDPS 2025)","year":2025},{"authors":["J. Ye","B. Nicolae","A. Kougkas","X.-H. Sun"],"date":"November, 2024","links":{"extended abstract":"http://cs.iit.edu/~scs/assets/files/ye2024kvcache_abstract.pdf","poster":"http://cs.iit.edu/~scs/assets/files/ye2024kvcache_poster.pdf"},"month":11,"slug":"ye-2024-uncover-overhead-3814","tags":["KV Cache","LLM Inference"],"title":"Uncover the Overhead and Resource Usage for Handling KV Cache Overflow in LLM Inference","type":"Poster","venue":"The International Conference for High Performance Computing, Networking, Storage, and Analysis (SC\'24)","year":2024},{"abstract":"The combination of ever-growing scientific datasets\\nand distributed workflow complexity creates I/O performance\\nbottlenecks due to data volume, velocity, and variety. Although\\nthe increasing use of descriptive data formats (e.g., HDF5,\\nnetCDF) helps organize these datasets, it also introduces obscure\\nbottlenecks due to the need to translate high-level operations\\ninto file addresses and then into low-level I/O operations. To\\naddress this challenge, we introduce DaYu, a method and toolset\\nfor analyzing (a) semantic relationships between logical datasets\\nand file addresses, (b) how dataset operations translate into\\nI/O, and (c) the combination across entire workflows. DaYu\'s\\nanalysis and visualization enable the identification of critical\\nbottlenecks and the reasoning about remediation. We describe\\nour methodology and propose optimization guidelines. Evaluation\\non scientific workflows demonstrates up to a 3.7x performance\\nimprovement in I/O time for obscure bottlenecks. The time and\\nstorage overhead for DaYu\'s time-ordered data are typically\\nunder 0.2% of runtime and 0.25% of data volume, respectively.","authors":["M. Tang","J. Cernuda","J. Ye","L. Guo","N. Tallent","A. Kougkas","X.-H. Sun"],"date":"September, 2024","doi":"10.1109/cluster59578.2024.00038","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/tang2024dayu.bib","citation":"http://cs.iit.edu/~scs/assets/files/tang2024dayu.txt","pdf":"http://cs.iit.edu/~scs/assets/files/tang2024dayu.pdf"},"month":9,"slug":"tang-2024-dayu-f286","tags":["Data Analytics","Modeling","Performance Measurement","Tools","Workflow Optimization"],"title":"DaYu: Optimizing Distributed Scientific Workflows by Decoding Dataflow Semantics and Dynamics","type":"Conference","venue":"2024 IEEE International Conference on Cluster Computing (CLUSTER\'24)","year":2024},{"abstract":"Data streaming is gaining traction in high-performance computing\\n(HPC) as a mechanism for continuous data transfer, but remains\\nunderutilized as a processing paradigm due to the inadequacy of\\nexisting technologies, which are primarily designed for cloud archi-\\ntectures and ill-equipped to tackle HPC-specific challenges. This\\nwork introduces HStream, a novel data management design for\\nout-of-core data streaming engines. Central to the HStream design\\nis the separation of data and computing planes at the task level. By\\nmanaging them independently, issues such as memory thrashing\\nand back-pressure, caused by the high volume, velocity, and bursti-\\nness of I/O in HPC environments, can be effectively addressed at\\nruntime. Specifically, HStream utilizes adaptive parallelism and hi-\\nerarchical memory management, enabled by this design paradigm,\\nto alleviate memory pressure and enhance system performance.\\nThese improvements enable HStream to match the performance of\\nstate-of-the-art HPC streaming engines and achieve up to a 1.5x\\nreduction in latency under high data loads.","authors":["J. Cernuda","J. Ye","A. Kougkas","X.-H. Sun"],"date":"August, 2024","doi":"10.1145/3673038.3673150","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/cernuda2024hstream.bib","citation":"http://cs.iit.edu/~scs/assets/files/cernuda2024hstream.txt","pdf":"http://cs.iit.edu/~scs/assets/files/cernuda2024hstream.pdf"},"month":8,"slug":"cernuda-2024-hstream-3043","tags":["Data Streaming","HPC","Hierarchical Storage","Elastic System","In-Transit Computing","LABIOS"],"title":"HStream: A hierarchical data streaming engine for high-throughput scientific applications","type":"Conference","venue":"The 53th International Conference on Parallel Processing (ICPP\'24)","year":2024},{"abstract":"Scientific workflows increasingly need to train a DNN model in\\nreal-time during an experiment (e.g. using ground truth from a sim-\\nulation), while using it at the same time for inferences. Instead of\\nsharing the same model instance, the training (producer) and infer-\\nence server (consumer) often use different model replicas that are\\nkept synchronized. In addition to efficient I/O techniques to keep the\\nmodel replica of the producer and consumer synchronized, there is\\nanother important trade-off: frequent model updates enhance infer-\\nence quality but may slow down training; infrequent updates may\\nlead to less precise inference results. To address these challenges, we\\nintroduce Viper: a new I/O framework designed to determine a near-\\noptimal checkpoint schedule and accelerate the delivery of the latest\\nmodel updates. Viper builds an inference performance predictor to\\nidentify the optimal checkpoint schedule to balance the trade-off be-\\ntween training slowdown and inference quality improvement. It also\\ncreates a memory-first model transfer engine to accelerate model\\ndelivery through direct memory-to-memory communication. Our\\nexperiments show that Viper can reduce the model update latency\\nby \u2248 9x using the GPU-to-GPU data transfer engine and \u2248 3x using\\nthe DRAM-to-DRAM host data transfer. The checkpoint schedule\\nobtained from Viper\'s predictor also demonstrates improved cumu-\\nlative inference accuracy compared to the baseline of epoch-based\\nsolutions.","authors":["J. Ye","J. Cernuda","N. Rajesh","K. Bateman","O. Yildiz","T. Peterka","A. Nigmetov","D. Morozov","A. Kougkas","X.-H. Sun","B. Nicolae"],"date":"August, 2024","doi":"10.1145/3673038.3673070","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/ye2024viper.bib","citation":"http://cs.iit.edu/~scs/assets/files/ye2024viper.txt","pdf":"http://cs.iit.edu/~scs/assets/files/ye2024viper.pdf","slides":"http://cs.iit.edu/~scs/assets/files/ye2024viper.pptx"},"month":8,"slug":"ye-2024-viper-0a1b","tags":["AI Workflows","Adaptive AI Model Checkpointing","Coupled Training and Inferences","Inferences During Partial Training","LABIOS"],"title":"Viper: A High-Performance I/O Framework for Transparently Updating, Storing, and Transferring Deep Neural Network Models","type":"Conference","venue":"The 53th International Conference on Parallel Processing (ICPP\'24)","year":2024},{"abstract":"Abstract-Storage in HPC is typically a single Remote and Static\\nStorage (RSS) resource. However, applications demonstrate diverse I/O\\nrequirements that can be better served by a multi-storage approach. Cur-\\nrent practice employs ephemeral storage systems running on either node-\\nlocal or shared storage resources. Yet, the burden of provisioning and\\nconfiguring intermediate storage falls solely on the users, while global job\\nschedulers offer little to no support for custom deployments. This lack of\\nsupport often leads to over- or under-provisioning of resources and poorly\\nconfigured storage systems. To mitigate this, we present LuxIO, an intelli-\\ngent storage resource provisioning and auto-configuration service. LuxIO\\nconstructs storage deployments configured to best match I/O require-\\nments. LuxIO-tuned storage services show performance improvements\\nup to 2x across common applications and benchmarks, while introducing\\nminimal overhead of 93.40 ms on top of existing job scheduling pipelines.\\nLuxIO improves resource utilization by up to 25% in select workflows.\\n","authors":["K. Bateman","N. Rajesh","J. Cernuda","L. Logan","J. Ye","S. Herbein","A. Kougkas","X.-H. Sun"],"date":"December, 2022","doi":"10.1109/hipc56025.2022.00041","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/bateman2022luxio.bib","citation":"http://cs.iit.edu/~scs/assets/files/bateman2022luxio.txt","pdf":"http://cs.iit.edu/~scs/assets/files/bateman2022luxio.pdf"},"month":12,"slug":"bateman-2022-luxio-2487","tags":["Resource Provisioning","I/O Behavior","Storage Auto-Tuning","ChronoLog"],"title":"LuxIO: Intelligent Resource Provisioning and Auto-Configuration for Storage Services","type":"Conference","venue":"The 29th edition of the IEEE International Conference on High Performance Computing, Data, and Analytics (HiPC\'22)","year":2022},{"authors":["J. Ye","A. Kougkas","X.-H. Sun"],"date":"November, 2021","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/ye2021hdf5vol_poster.bib","citation":"http://cs.iit.edu/~scs/assets/files/ye2021hdf5vol_poster.txt","extended abstract":"http://cs.iit.edu/~scs/assets/files/ye2021hdf5vol_abstract.pdf","poster":"http://cs.iit.edu/~scs/assets/files/ye2021hdf5vol_poster.pdf"},"month":11,"slug":"ye-2021-hdf5-vol-4754","tags":["HDF5","Apache Arrow","Column store"],"title":"HDF5 VOL Connector to Apache Arrow","type":"Poster","venue":"The 2021 International Conference for High Performance Computing, Networking, Storage, and Analysis (SC\'21), November 14\u201319, 2021","year":2021},{"abstract":"Modern applications are highly data-intensive, leading\\nto the well-known I/O bottleneck problem. Scientists have proposed\\nthe placement of fast intermediate storage resources which aim\\nto mask the I/O penalties. To manage these resources, three core\\nsoftware abstractions are being used in leadership-class computing\\nfacilities: IO Forwarders, Burst Buffers, and Data Stagers. Yet, with\\nthe rise of multi-tenant deployment in HPC systems, these software\\nabstractions are: managed and maintained in isolation, leading to\\ninefficient interactions; allocated statically, leading to load imbalance;\\nexclusively bifurcated between the intermediate storage, leading to\\nunder-utilization of resources, and, in many cases, do not support\\nin-situ operations. To this end, we present HFlow, a new class of\\ndata forwarding system that leverages a real-time data movement\\nparadigm. HFlow introduces a unified data movement abstraction\\n(the ByteFlow) providing data-independent tasks that can be\\nexecuted anywhere and thus, enabling dynamic resource provisioning.\\nMoreover, the processing elements executing the ByteFlows are\\ndesigned to be ephemeral and, hence, enable elastic management of\\nintermediate storage resources. Our results show that applications\\nrunning under HFlow display an increase in performance of 3x when\\ncompared with state-of-the-art software solutions.\\nIndex Terms-Data streaming, I/O forwarding, elasticity,\\ndynamicity, multi-tenant, data-intensive, I/O,data pipeline, in-transit","authors":["J. Cernuda","H. Devarajan","L. Logan","K. Bateman","N. Rajesh","J. Ye","A. Kougkas","X.-H. Sun"],"date":"September, 2021","doi":"10.1109/cluster48925.2021.00064","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/cernuda2021HFlow.bib","citation":"http://cs.iit.edu/~scs/assets/files/cernuda2021HFlow.txt","pdf":"http://cs.iit.edu/~scs/assets/files/cernuda2021HFlow.pdf"},"month":9,"slug":"cernuda-2021-hflow-2f5b","tags":["Hermes"],"title":"HFlow: A Dynamic and Elastic Multi-Layered Data Forwarder","type":"Conference","venue":"The 2021 IEEE International Conference on Cluster Computing (CLUSTER\'21), September 7-10, 2021","year":2021},{"abstract":"Applications and middleware services, such as data placement en-\\ngines, I/O scheduling, and prefetching engines, require low-latency\\naccess to telemetry data in order to make optimal decisions. However,\\ntypical monitoring services store their telemetry data in a database\\nin order to allow applications to query them, resulting in significant\\nlatency penalties. This work presents Apollo: a low-latency mon-\\nitoring service that aims to provide applications and middleware\\nlibraries with direct access to relational telemetry data. Monitoring\\nthe system can create interference and overhead, slowing down raw\\nperformance of the resources for the job. However, having a current\\nview of the system can aid middleware services in making more\\noptimal decisions which can ultimately improve the overall perfor-\\nmance. Apollo has been designed from the ground up to provide\\nlow latency, using Publish\u2013Subscribe (Pub-Sub) semantics, and low\\noverhead, using adaptive intervals in order to change the length of\\ntime between polling the resource for telemetry data and machine\\nlearning in order to predict changes to the telemetry data between\\nactual resource polling. This work also provides some high level\\nabstractions called I/O curators, which can further aid middleware\\nlibraries and applications to make optimal decisions. Evaluations\\nshowcase that Apollo can achieve sub-millisecond latency for acquir-\\ning complex insights with a memory overhead of ~57MB and CPU\\noverhead being only 7% more than existing state-of-the-art systems.","authors":["N. Rajesh","H. Devarajan","J. Cernuda","K. Bateman","L. Logan","J. Ye","A. Kougkas","X.-H. Sun"],"date":"June, 2021","doi":"10.1145/3431379.3460640","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/rajesh2021apollo.bib","citation":"http://cs.iit.edu/~scs/assets/files/rajesh2021apollo.txt","pdf":"http://cs.iit.edu/~scs/assets/files/rajesh2021apollo.pdf"},"month":6,"slug":"rajesh-2021-apollo-dbd8","tags":["HPC","Machine Learning","Resource Monitoring","Hermes"],"title":"Apollo: An ML-assisted Real-Time Storage Resource Observer","type":"Conference","venue":"The 30th ACM International Symposium on High-Performance Parallel and Distributed Computing (HPDC\'21), June 21-25, 2021","year":2021}]')}}]);