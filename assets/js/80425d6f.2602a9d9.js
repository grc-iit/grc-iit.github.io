"use strict";(self.webpackChunkgrc=self.webpackChunkgrc||[]).push([[7706],{57944:e=>{e.exports=JSON.parse('[{"abstract":"HPC, Big Data Analytics, and Machine Learning have become in-\\ncreasingly intertwined as popular models such as LLMs and Diffusion\\nModels have been driving discovery in scientific fields. However,\\neach of these domains has its own storage infrastructure with unique\\nI/O interfaces and storage systems, requiring feature sets that are\\noften incompatible. Users with experience in one domain lack the\\nexpertise to change their applications to match the data stacks of\\nthe other domains, necessitating expensive conversions. There is\\na need for a transparent solution for the unification of disparate\\ndata stacks for the triple convergence of HPC, Big Data, and ML\\nthat can provide the required functionality while achieving higher\\nperformance. To better support converged HPC, Big Data, and ML\\nworkflows, this paper proposes DTIO, a scalable I/O runtime that\\nunifies the disparate I/O stack for modern scientific ML workflows.\\nDTIO utilizes a unique DataTask abstraction to express the move-\\nment of data, its ordering, and its dependencies on other data as a\\ntask. DTIO achieves a unification of scientific and ML workflows\\nby utilizing intelligent mapping of interfaces, and automatically de-\\ntermines the best method to relate their unique semantics. DTIO\'s\\nonline translation with DataTask caching can improve performance\\nby 49.6% compared to offline translation methods. DTIO also offers\\nnumerous optimizations, such as asynchronous I/O and aggregation.","authors":["K. Bateman","N. Rajesh","J. Cernuda","L. Logan","B. Nicolae","F. Cappello","X.-H. Sun","A. Kougkas"],"date":"June, 2025","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/bateman2025dtio.bib","citation":"http://cs.iit.edu/~scs/assets/files/bateman2025dtio.txt","pdf":"http://cs.iit.edu/~scs/assets/files/bateman2025dtio.pdf"},"month":6,"slug":"bateman-2025-dtio-e77d","tags":["Task Systems","Data Stacks","Systems for AI Workflows"],"title":"DTIO: Data Stack for AI-driven Workflows","type":"Conference","venue":"The 37th International Conference on Scalable Scientific Data Management (SSDBM 2025)","year":2025},{"abstract":"Hardware is becoming increasingly heterogeneous in\\nmodern high-performance computing clusters. However, computing\\nenvironments for developing tools to harness these technologies are\\nnot easily available to researchers. This work showcases the need for\\na new high-pace, heterogeneous I/O research cluster and presents a\\nnovel software deployment framework named Jarvis to manage its\\nhardware diversity. Jarvis is an extensible Python framework that\\nallows users to create Packages that deploy, manage, and monitor\\nsoftware, including complex applications (e.g., scientific simulations),\\nsupport tools (e.g., Darshan, GDB), and storage systems (e.g.,\\nLustre, DAOS). These packages can be combined to form complex\\ndeployment Pipelines. To ensure pipelines are portable across\\nhardware, Jarvis defines a novel Resource Graph schema file,\\nwhich is a snapshot of a cluster\'s machine-specific information.\\nThis schema can be queried by Jarvis packages to deploy software\\nacross diverse hardware compositions with minimal user effort.\\nIndex Terms-deployment, HPC, hardware abstraction, resource\\nmanagement, I/O, Python","authors":["J. Cernuda","L. Logan","N. Lewis","S. Byna","X.-H. Sun","A. Kougkas"],"date":"November, 2024","links":{"pdf":"http://cs.iit.edu/~scs/assets/files/cernuda2024jarvis.pdf"},"month":11,"slug":"cernuda-2024-jarvis-3b52","tags":["Deployment","HPC","Hardware Abstraction","I/O","Python","Resource Management"],"title":"Jarvis: Towards a Shared, User-Friendly, and Reproducible, I/O Infrastructure.","type":"Workshop","venue":"The International Parallel Data Systems Workshop (PDSW\'24)","year":2024},{"abstract":"Large-scale data analytics, scientific simulation, and\\ndeep learning codes in HPC perform massive computations on data\\ngreatly exceeding the bounds of main memory. These out-of-core al-\\ngorithms suffer from severe data movement penalties, programming\\ncomplexity, and limited code reuse. To solve this, HPC sites have\\nsteadily increased DRAM capacity. However, this is not sustainable\\ndue to financial and environmental costs. A more elegant, low-cost,\\nand portable solution is to expand memory to distributed multi-\\ntiered storage. In this work, we propose MegaMmap: a software\\ndistributed shared memory (DSM) that enlarges effective memory\\ncapacity through intelligent tiered DRAM and storage management.\\nMegaMmap provides workload-aware data organization, eviction,\\nand prefetching policies to reduce DRAM consumption while ensur-\\ning speedy access to critical data. A variety of memory coherence\\noptimizations are provided through an intuitive hinting system.\\nEvaluations show that various workloads can be executed with a\\nfraction of the DRAM while offering competitive performance.","authors":["L. Logan","X.-H. Sun","A. Kougkas"],"date":"November, 2024","doi":"10.1109/sc41406.2024.00114","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/logan2024megammap.bib","citation":"http://cs.iit.edu/~scs/assets/files/logan2024megammap.txt","pdf":"http://cs.iit.edu/~scs/assets/files/logan2024megammap.pdf"},"month":11,"slug":"logan-2024-megammap-cbda","tags":["Memory","Storage","Hierarchical Storage","HPC","Operating Systems","LABIOS"],"title":"MegaMmap: Blurring the Boundary Between Memory and Storage for Data-Intensive Workloads","type":"Conference","venue":"The International Conference for High Performance Computing, Networking, Storage, and Analysis (SC\'24)","year":2024},{"abstract":"Traditionally, distributed storage systems have relied upon\\nthe interfaces provided by OS kernels to interact with stor-\\nage hardware. However, much research has shown that OSes\\nimpose serious overheads on every I/O operation, especially\\non high-performance storage and networking hardware (e.g.,\\nPMEM and 200GBe). Thus, distributed storage stacks are\\nbeing re-designed to take advantage of this modern hard-\\nware by utilizing new hardware interfaces which bypass the\\nkernel entirely. However, the impact of these optimizations\\nhave not been well-studied for real HPC workloads on real\\nhardware. In this work, we provide a comprehensive evalua-\\ntion of DAOS: a state-of-the-art distributed storage system\\nwhich re-architects the storage stack from scratch for mod-\\nern hardware. We compare DAOS against traditional storage\\nstacks and demonstrate that by utilizing optimal interfaces\\nto hardware, performance improvements of up to 6x can be\\nobserved in real scientific applications.","authors":["L. Logan","J. Lofstead","A. Kougkas","X.-H. Sun"],"date":"June, 2024","doi":"10.1145/3578353.3589542","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/logan2024daos.bib","citation":"http://cs.iit.edu/~scs/assets/files/logan2024daos.txt","pdf":"http://cs.iit.edu/~scs/assets/files/logan2023daos.pdf"},"month":6,"slug":"logan-2024-evaluation-daos-6eb2","tags":["Distributed Computing","Distributed Storage","Flash Memory","Machine Learning","Parallel Computing","Phase Change Memory"],"title":"An Evaluation of DAOS for Simulation and Deep Learning HPC Workloads","type":"Journal","venue":"SIGOPS Operating Systems Review (OSR\'24)","year":2024},{"abstract":"Modern simulation workflows generate and analyze\\nmassive amounts of data using I/O libraries like Adios2 and\\nNetCDF. Although extensive work has optimized the I/O\\nprocesses during the simulation phase, executing analytical\\nqueries-which often require iterative traversals of large files\\nfor insights-is cumbersome and usually constrained by low I/O\\nperformance. Instead of waiting for the analysis phase to process\\nqueries, quantities can be derived asynchronously during data\\nproduction and cached, speeding up future queries. In this\\nwork, we introduce a context-aware I/O layer named \'Hades.\' It\\nis designed to efficiently derive insights from selected quantities\\nwithout compromising overall workflow performance. Hades\\nactively and asynchronously computes and stores these quantities\\nwhile the data is in transit. Hades leverages a hierarchical\\nbuffering system with data access-aware prefetching to ensure\\nquick and timely access to relevant data. It offers a flexible\\nquery interface empowering users to easily define derived\\nquantities and provide control over data placement decisions.\\nHades is implemented using an Adios2 plugin engine and the\\nHermes buffering platform, enabling transparent use by any\\nAdios-powered application or workflow. Experimental results\\ndemonstrate performance improvements by up to 3-4x for tested\\nreal-world scientific producer-consumer workflows.\\nIndex Terms-Active Storage, Hierarchical Storage, Context\\nAwareness, Metadata Management, Data Operator, In-transit\\nComputing","authors":["J. Cernuda","L. Logan","A. Gainaru","J. Lofstead","A. Kougkas","X.-H. Sun"],"date":"May, 2024","doi":"10.1109/ccgrid59990.2024.00070","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/cernuda2024hades.bib","citation":"http://cs.iit.edu/~scs/assets/files/cernuda2024hades.txt","pdf":"http://cs.iit.edu/~scs/assets/files/cernuda2024hades.pdf"},"month":5,"slug":"cernuda-2024-hades-e18c","tags":["Active Storage","Hierarchical Storage","Context Awareness","Metadata Management","Data Operator","In-Transit Computing","Coeus"],"title":"Hades: A Context-Aware Active Storage Framework for Accelerating Large-Scale Data Analysis","type":"Conference","venue":"The 24th IEEE/ACM international Symposium on Cluster, Cloud and Internet Computing (CCGRID 2024)","year":2024},{"abstract":"Traditionally, distributed storage systems have relied upon\\nthe interfaces provided by OS kernels to interact with stor-\\nage hardware. However, much research has shown that OSes\\nimpose serious overheads on every I/O operation, especially\\non high-performance storage and networking hardware (e.g.,\\nPMEM and 200GBe). Thus, distributed storage stacks are\\nbeing re-designed to take advantage of this modern hard-\\nware by utilizing new hardware interfaces which bypass the\\nkernel entirely. However, the impact of these optimizations\\nhave not been well-studied for real HPC workloads on real\\nhardware. In this work, we provide a comprehensive evalua-\\ntion of DAOS: a state-of-the-art distributed storage system\\nwhich re-architects the storage stack from scratch for mod-\\nern hardware. We compare DAOS against traditional storage\\nstacks and demonstrate that by utilizing optimal interfaces\\nto hardware, performance improvements of up to 6x can be\\nobserved in real scientific applications.","authors":["L. Logan","J. Lofstead","A. Kougkas","X.-H. Sun"],"date":"May, 2023","doi":"10.1145/3578353.3589542","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/logan2023daos.bib","citation":"http://cs.iit.edu/~scs/assets/files/logan2023daos.txt","pdf":"http://cs.iit.edu/~scs/assets/files/logan2023daos.pdf"},"month":5,"slug":"logan-2023-evaluation-daos-a960","tags":["Distributed Computing","Distributed Storage","Flash Memory","Machine Learning","Parallel Computing","Phase Change Memory"],"title":"An Evaluation of DAOS for Simulation and Deep Learning HPC Workloads","type":"Workshop","venue":"The 3rd Workshop on Challenges and Opportunities of Efficient and Performant Storage Systems (CHEOPS\'23)","year":2023},{"abstract":"Abstract-Storage in HPC is typically a single Remote and Static\\nStorage (RSS) resource. However, applications demonstrate diverse I/O\\nrequirements that can be better served by a multi-storage approach. Cur-\\nrent practice employs ephemeral storage systems running on either node-\\nlocal or shared storage resources. Yet, the burden of provisioning and\\nconfiguring intermediate storage falls solely on the users, while global job\\nschedulers offer little to no support for custom deployments. This lack of\\nsupport often leads to over- or under-provisioning of resources and poorly\\nconfigured storage systems. To mitigate this, we present LuxIO, an intelli-\\ngent storage resource provisioning and auto-configuration service. LuxIO\\nconstructs storage deployments configured to best match I/O require-\\nments. LuxIO-tuned storage services show performance improvements\\nup to 2x across common applications and benchmarks, while introducing\\nminimal overhead of 93.40 ms on top of existing job scheduling pipelines.\\nLuxIO improves resource utilization by up to 25% in select workflows.\\n","authors":["K. Bateman","N. Rajesh","J. Cernuda","L. Logan","J. Ye","S. Herbein","A. Kougkas","X.-H. Sun"],"date":"December, 2022","doi":"10.1109/hipc56025.2022.00041","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/bateman2022luxio.bib","citation":"http://cs.iit.edu/~scs/assets/files/bateman2022luxio.txt","pdf":"http://cs.iit.edu/~scs/assets/files/bateman2022luxio.pdf"},"month":12,"slug":"bateman-2022-luxio-2487","tags":["Resource Provisioning","I/O Behavior","Storage Auto-Tuning","ChronoLog"],"title":"LuxIO: Intelligent Resource Provisioning and Auto-Configuration for Storage Services","type":"Conference","venue":"The 29th edition of the IEEE International Conference on High Performance Computing, Data, and Analytics (HiPC\'22)","year":2022},{"abstract":"Traditionally, I/O systems have been developed within\\nthe confines of a centralized OS kernel. This led to monolithic\\nand rigid storage systems that are limited by low development\\nspeed, expressiveness, and performance. Various assumptions are\\nimposed including reliance on the UNIX-file abstraction, the POSIX\\nstandard, and a narrow set of I/O policies. However, this monolithic\\ndesign philosophy makes it difficult to develop and deploy new\\nI/O approaches to satisfy the rapidly-evolving I/O requirements of\\nmodern scientific applications. To this end, we propose LabStor: a\\nmodular and extensible platform for developing high-performance,\\ncustomized I/O stacks. Single-purpose I/O modules (e.g, I/O\\nschedulers) can be developed in the comfort of userspace and released\\nas plug-ins, while end-users can compose these modules to form\\nworkload- and hardware-specific I/O stacks. Evaluations show that\\nby switching to a fully modular design, tailored I/O stacks can yield\\nperformance improvements of up to 60% in various applications.","authors":["L. Logan","J. Cernuda","J. Lofstead","X.-H. Sun","A. Kougkas"],"date":"November, 2022","doi":"10.1109/sc41404.2022.00028","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/logan2022labstor.bib","citation":"http://cs.iit.edu/~scs/assets/files/logan2022labstor.txt","pdf":"http://cs.iit.edu/~scs/assets/files/logan2022labstor.pdf","slides":"http://cs.iit.edu/~scs/assets/files/logan2022labstor_slides.pdf"},"month":11,"slug":"logan-2022-labstor-f69d","tags":["Clouds and Distributed Computing","Programming Frameworks","System Software","ChronoLog"],"title":"LabStor: A Modular and Extensible Platform for Developing High-Performance, Customized I/O Stacks in Userspace","type":"Conference","venue":"The International Conference for High Performance Computing, Networking, Storage, and Analysis (SC\'22)","year":2022},{"authors":["L. Logan","J. Lofstead","S. Levy","P. Widener","X.-H. Sun","A. Kougkas"],"date":"November, 2021","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/logan2021pmemcpy_poster.bib","citation":"http://cs.iit.edu/~scs/assets/files/logan2021pmemcpy_poster.txt","extended abstract":"http://cs.iit.edu/~scs/assets/files/logan2021pmemcpy_abstract.pdf","poster":"http://cs.iit.edu/~scs/assets/files/logan2021pmemcpy_poster.pdf"},"month":11,"slug":"logan-2021-utilizing-persistent-24d5","tags":["Persistent Memory","Libraries","Memory Management","Memory Mapped I/O"],"title":"Utilizing Persistent Memory in Parallel I/O Libraries","type":"Poster","venue":"The 2021 International Conference for High Performance Computing, Networking, Storage, and Analysis (SC\'21), November 14\u201319, 2021","year":2021},{"abstract":"Modern applications are highly data-intensive, leading\\nto the well-known I/O bottleneck problem. Scientists have proposed\\nthe placement of fast intermediate storage resources which aim\\nto mask the I/O penalties. To manage these resources, three core\\nsoftware abstractions are being used in leadership-class computing\\nfacilities: IO Forwarders, Burst Buffers, and Data Stagers. Yet, with\\nthe rise of multi-tenant deployment in HPC systems, these software\\nabstractions are: managed and maintained in isolation, leading to\\ninefficient interactions; allocated statically, leading to load imbalance;\\nexclusively bifurcated between the intermediate storage, leading to\\nunder-utilization of resources, and, in many cases, do not support\\nin-situ operations. To this end, we present HFlow, a new class of\\ndata forwarding system that leverages a real-time data movement\\nparadigm. HFlow introduces a unified data movement abstraction\\n(the ByteFlow) providing data-independent tasks that can be\\nexecuted anywhere and thus, enabling dynamic resource provisioning.\\nMoreover, the processing elements executing the ByteFlows are\\ndesigned to be ephemeral and, hence, enable elastic management of\\nintermediate storage resources. Our results show that applications\\nrunning under HFlow display an increase in performance of 3x when\\ncompared with state-of-the-art software solutions.\\nIndex Terms-Data streaming, I/O forwarding, elasticity,\\ndynamicity, multi-tenant, data-intensive, I/O,data pipeline, in-transit","authors":["J. Cernuda","H. Devarajan","L. Logan","K. Bateman","N. Rajesh","J. Ye","A. Kougkas","X.-H. Sun"],"date":"September, 2021","doi":"10.1109/cluster48925.2021.00064","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/cernuda2021HFlow.bib","citation":"http://cs.iit.edu/~scs/assets/files/cernuda2021HFlow.txt","pdf":"http://cs.iit.edu/~scs/assets/files/cernuda2021HFlow.pdf"},"month":9,"slug":"cernuda-2021-hflow-2f5b","tags":["Hermes"],"title":"HFlow: A Dynamic and Elastic Multi-Layered Data Forwarder","type":"Conference","venue":"The 2021 IEEE International Conference on Cluster Computing (CLUSTER\'21), September 7-10, 2021","year":2021},{"abstract":"Persistent memory (PMEM) devices can achieve\\ncomparable performance to DRAM while providing significantly\\nmore capacity. This has made the technology compelling as\\nan expansion to main memory. Rethinking PMEM as storage\\ndevices can offer a high performance buffering layer for HPC\\napplications to temporarily, but safely store data. However,\\nmodern parallel I/O libraries, such as HDF5 and pNetCDF, are\\ncomplicated and introduce significant software and metadata\\noverheads when persisting data to these storage devices, wasting\\nmuch of their potential. In this work, we explore the potential\\nof PMEM as storage through pMEMCPY: a simple, lightweight,\\nand portable I/O library for storing data in persistent memory.\\nWe demonstrate that our approach is up to 2x faster than other\\npopular parallel I/O libraries under real workloads.","authors":["L. Logan","J. Lofstead","S. Levy","P. Widener","X.-H. Sun","A. Kougkas"],"date":"September, 2021","doi":"10.1109/cluster48925.2021.00098","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/logan2021pmemcpy.bib","citation":"http://cs.iit.edu/~scs/assets/files/logan2021pmemcpy.txt","pdf":"http://cs.iit.edu/~scs/assets/files/logan2021pmemcpy.pdf"},"month":9,"slug":"logan-2021-pmemcpy-f81c","tags":["Persistent Memory","Libraries","Memory Management","Memory Mapped I/O"],"title":"pMEMCPY: a simple, lightweight, and portable I/O library for storing data in persistent memory","type":"Workshop","venue":"The 1st Workshop on Re-envisioning Extreme-Scale I/O for Emerging Hybrid HPC Workloads (REX-IO\'21), in conjunction with the 2021 IEEE International Conference on Cluster Computing (CLUSTER\'21), September 7-10, 2021","year":2021},{"abstract":"Applications and middleware services, such as data placement en-\\ngines, I/O scheduling, and prefetching engines, require low-latency\\naccess to telemetry data in order to make optimal decisions. However,\\ntypical monitoring services store their telemetry data in a database\\nin order to allow applications to query them, resulting in significant\\nlatency penalties. This work presents Apollo: a low-latency mon-\\nitoring service that aims to provide applications and middleware\\nlibraries with direct access to relational telemetry data. Monitoring\\nthe system can create interference and overhead, slowing down raw\\nperformance of the resources for the job. However, having a current\\nview of the system can aid middleware services in making more\\noptimal decisions which can ultimately improve the overall perfor-\\nmance. Apollo has been designed from the ground up to provide\\nlow latency, using Publish\u2013Subscribe (Pub-Sub) semantics, and low\\noverhead, using adaptive intervals in order to change the length of\\ntime between polling the resource for telemetry data and machine\\nlearning in order to predict changes to the telemetry data between\\nactual resource polling. This work also provides some high level\\nabstractions called I/O curators, which can further aid middleware\\nlibraries and applications to make optimal decisions. Evaluations\\nshowcase that Apollo can achieve sub-millisecond latency for acquir-\\ning complex insights with a memory overhead of ~57MB and CPU\\noverhead being only 7% more than existing state-of-the-art systems.","authors":["N. Rajesh","H. Devarajan","J. Cernuda","K. Bateman","L. Logan","J. Ye","A. Kougkas","X.-H. Sun"],"date":"June, 2021","doi":"10.1145/3431379.3460640","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/rajesh2021apollo.bib","citation":"http://cs.iit.edu/~scs/assets/files/rajesh2021apollo.txt","pdf":"http://cs.iit.edu/~scs/assets/files/rajesh2021apollo.pdf"},"month":6,"slug":"rajesh-2021-apollo-dbd8","tags":["HPC","Machine Learning","Resource Monitoring","Hermes"],"title":"Apollo: An ML-assisted Real-Time Storage Resource Observer","type":"Conference","venue":"The 30th ACM International Symposium on High-Performance Parallel and Distributed Computing (HPDC\'21), June 21-25, 2021","year":2021},{"authors":["L. Logan","A. Kougkas","X.-H. Sun"],"date":"November, 2020","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/logan2020quantifying.bib","citation":"http://cs.iit.edu/~scs/assets/files/logan2020quantifying.txt","extended abstract":"http://cs.iit.edu/~scs/assets/files/logan2020quantifying-abstract.pdf","poster":"http://cs.iit.edu/~scs/assets/files/logan2020quantifying-poster.pdf"},"month":11,"slug":"logan-2020-quantifying-overheads-cfc2","tags":["I/O Bottleneck","Filesystems","Linux"],"title":"Quantifying the Overheads of the Modern Linux I/O Stack","type":"Poster","venue":"The International Conference for High Performance Computing, Networking, Storage and Analysis (SC\'20)","year":2020},{"abstract":"Modern scientific applications read and write\\nmassive amounts of data through simulations, observations, and\\nanalysis. These applications spend the majority of their runtime\\nin performing I/O. HPC storage solutions include fast node-local\\nand shared storage resources to elevate applications from this\\nbottleneck. Moreover, several middleware libraries (e.g., Hermes)\\nare proposed to move data between these tiers transparently.\\nData reduction is another technique that reduces the amount of\\ndata produced and, hence, improve I/O performance. These two\\ntechnologies, if used together, can benefit from each other. The\\neffectiveness of data compression can be enhanced by selecting\\ndifferent compression algorithms according to the characteristics\\nof the different tiers, and the multi-tiered hierarchy can benefit\\nfrom extra capacity. In this paper, we design and implement\\nHCompress, a hierarchical data compression library that\\ncan improve the application\'s performance by harmoniously\\nleveraging both multi-tiered storage and data compression.\\nWe have developed a novel compression selection algorithm\\nthat facilitates the optimal matching of compression libraries\\nto the tiered storage. Our evaluation shows that HCompress\\ncan improve scientific application\'s performance by 7x when\\ncompared to other state-of-the-art tiered storage solutions.","authors":["H. Devarajan","A. Kougkas","L. Logan","X.-H. Sun"],"date":"May, 2020","doi":"10.1109/ipdps47924.2020.00064","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/hcompress2020.bib","citation":"http://cs.iit.edu/~scs/assets/files/hcompress2020.txt","pdf":"http://cs.iit.edu/~scs/assets/files/hcompress.pdf","slides":"http://cs.iit.edu/~scs/assets/files/hcompress_ipdps.pdf"},"month":5,"slug":"devarajan-2020-hcompress-70f7","tags":["Hierarchical","Multi-Tiered","Data Compression","Data-Reduction","Dynamic Choice","Workflow Priorities","Library","Middleware","Engine","Hermes"],"title":"HCompress: Hierarchical Data Compression for Multi-Tiered Storage Environments","type":"Conference","venue":"IEEE International Parallel and Distributed Processing Symposium (IPDPS\'20), May 18-22, 2020","year":2020}]')}}]);