"use strict";(self.webpackChunkgrc=self.webpackChunkgrc||[]).push([[11642],{10473:e=>{e.exports=JSON.parse('[{"authors":["H. Geng","X. Lu","Y. Che","Z. Tian","D. Cheng","X.-H. Sun","M. Niemier","X. Hu"],"date":"October, 2025","links":{},"month":10,"slug":"geng-2025-cosmos-fb4d","tags":["Cache Management","Memory System Optimization"],"title":"COSMOS: RL-Enhanced Locality-Aware Counter Cache Optimization for Secure Memory","type":"Conference","venue":"The 58th IEEE/ACM International Symposium on Microarchitecture (MICRO-58)","year":2025},{"authors":["Y. Wu","X. Lu","X. Chen","Y. Han","X.-H. Sun"],"date":"June, 2025","links":{},"month":6,"slug":"wu-2025-concurrency-aware-42dd","tags":["Cache Management","Memory System Optimization","Concurrency"],"title":"Concurrency-Aware Cache Miss Cost Prediction with Perceptron Learning","type":"Conference","venue":"The 35th ACM Great Lakes Symposium on VLSI (GLSVLSI 2025), New Orleans, LA, USA, June 30 - July 2, 2025","year":2025},{"authors":["L. Yan","X. Lu","S. Xu","X. Chen","X. Zou","Y. Han","X.-H. Sun"],"date":"May, 2025","links":{},"month":5,"slug":"yan-2025-prominer-7dc6","tags":["Graph Mining","Processing-in-Memory","Hardware Acceleration"],"title":"ProMiner: Enhancing Locality, Parallelism, and Offloading for Graph Mining on Processing-in-Memory Systems","type":"Journal","venue":"IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (TCAD), 2025","year":2025},{"authors":["L. Yan","X. Lu","X. Chen","Y. Han","X.-H. Sun"],"date":"April, 2025","links":{},"month":4,"slug":"yan-2025-pyramid-7e42","tags":["LLM Inference","Processing-in-Memory","Hardware Acceleration"],"title":"Pyramid: Accelerating LLM Inference with Cross-Level Processing-in-Memory","type":"Journal","venue":"IEEE Computer Architecture Letters (CAL), April 2025","year":2025},{"abstract":"Graph pattern matching (GPM), a critical algorithm for discovering specific patterns within complex structures, is becoming increasingly important in the data-driven world. GPM applications are memory-bound and can be accelerated by memory-centric computing systems, such as processing-in-memory (PIM). However, there are three primary challenges when it comes to accelerating GPM applications with PIM: (1) difficulty in utilizing locality, (2) heavy data movement, and (3) heavy comparison overhead due to pruning. To address these challenges, we propose AceMiner, a framework to accelerate GPM applications with a software and hardware co-design perspective using PIM. In AceMiner, we embed hybridCache, a novel in-DRAM cache system with lower access latency and optimized replacement policy, to leverage the potential locality and reduce data movement in PIM. Additionally, we introduce a comparison unit to address the huge pruning overhead. Experimental results show that AceMiner outperforms the state-of-the-art, achieving speedups of 40.2% and 13.3% over NDMiner and DIMMining respectively, with less energy consumption and design overhead.","authors":["L. Yan","X. Lu","X. Chen","S. Xu","X. Zou","Y. Han","X.-H. Sun"],"date":"November, 2024","doi":"10.1109/iccd63220.2024.00091","links":{"pdf":"http://cs.iit.edu/~scs/assets/files/yan2024aceminer.pdf","slides":"http://cs.iit.edu/~scs/assets/files/yan2024aceminer_slides.pdf"},"month":11,"slug":"yan-2024-aceminer-c390","tags":["Graph Pattern Matching","Processing-in-Memory","Cache System"],"title":"AceMiner: Accelerating Graph Pattern Matching using PIM with Optimized Cache System","type":"Conference","venue":"The 2024 IEEE 42nd International Conference on Computer Design (ICCD\'24)","year":2024},{"abstract":"Sparse matrix-matrix multiplication (SpMM) is a critical com-\\nputational kernel in numerous scientific and machine learn-\\ning applications. SpMM involves massive irregular memory\\naccesses and poses great challenges to conventional cache-\\nbased computer architectures. Recently dedicated SpMM\\naccelerators have been proposed to enhance SpMM per-\\nformance. However, current SpMM accelerators still face\\nchallenges in adapting to varied sparse patterns, fully ex-\\nploiting inherent parallelism, and optimizing cache perfor-\\nmance. To address these issues, we introduce ACES, a novel\\nSpMM accelerator in this study. First, ACES features an\\nadaptive execution flow that dynamically adjusts to diverse\\nsparse patterns. The adaptive execution flow balances par-\\nallel computing efficiency and data reuse. Second, ACES\\nincorporates locality-concurrency co-optimizations within\\nthe global cache. ACES utilizes a concurrency-aware cache\\nmanagement policy, which considers data locality and con-\\ncurrency for optimal replacement decisions. Additionally, the\\nintegration of a non-blocking buffer with the global cache en-\\nhances concurrency and reduces computational stalls. Third,\\nthe hardware architecture of ACES is designed to integrate\\nall innovations. The architecture ensures efficient support\\nacross the adaptive execution flow, advanced cache opti-\\nmizations, and fine-grained parallel processing. Our perfor-\\nmance evaluation demonstrates that ACES significantly out-\\nperforms existing solutions, providing a 2.1\xd7 speedup and\\nmarking a substantial advancement in SpMM acceleration.","authors":["X. Lu","B. Long","X. Chen","Y. Han","X.-H. Sun"],"date":"April, 2024","doi":"10.1145/3620666.3651381","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/lu2024aces.bib","citation":"http://cs.iit.edu/~scs/assets/files/lu2024aces.txt","pdf":"http://cs.iit.edu/~scs/assets/files/lu2024aces.pdf","poster":"http://cs.iit.edu/~scs/assets/files/lu2024aces_poster.pdf","slides":"http://cs.iit.edu/~scs/assets/files/lu2024aces_slides.pdf"},"month":4,"slug":"lu-2024-aces-6fdd","tags":["SpMM","Accelerator","Parallelism","Concurrency","Synchronization","Scalability","UniMCC"],"title":"ACES: Accelerating Sparse Matrix Multiplication with Adaptive Execution Flow and Concurrency-Aware Cache Optimizations","type":"Conference","venue":"The 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-29)","year":2024},{"abstract":"Cache management is a critical aspect of computer\\narchitecture, encompassing techniques such as cache replace-\\nment, bypassing, and prefetching. Existing research has often fo-\\ncused on individual techniques, overlooking the potential benefits\\nof joint optimization. Moreover, many of these approaches rely\\non static and intuition-driven policies, limiting their performance\\nunder complex and dynamic workloads. To address these chal-\\nlenges, this paper introduceesCHROME,a novelty\\naware cache management framework. CHROME takes a holistic\\napproach by seamlessly integrating intelligent cache replacement\\nand bypassing with pattern-based prefetching. By leveraging\\nonline reinforcement learning, CHROME dynamically adapts\\ncache decisions based on multiple program features and applies a\\nreward for each decision that considers the accuracy of the action\\nand the system-level feedback information. Our performance\\nevaluation demonstrates that CHROME outperforms current\\nstate-of-the-art schemes, exhibiting significant improvements in\\ncache management. Notably, CHROME achieves a remarkable\\nperformance boost of up to 13.7% over the traditional LRU\\nmethod in multi-core systems with only modest overhead.","authors":["X. Lu","H. Najafi","J. Liu","X.-H. Sun"],"date":"March, 2024","doi":"10.1109/hpca57654.2024.00090","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/lu2024chrome.bib","citation":"http://cs.iit.edu/~scs/assets/files/lu2024chrome.txt","pdf":"http://cs.iit.edu/~scs/assets/files/lu2024chrome.pdf","slides":"http://cs.iit.edu/~scs/assets/files/lu2024chrome_slides.pdf"},"month":3,"slug":"lu-2024-chrome-1321","tags":["Memory Architecture","Cache Management","Optimization of Memory Architectures","UniMCC"],"title":"CHROME: Concurrency-Aware Holistic Cache Management Framework with Online Reinforcement Learning","type":"Conference","venue":"The 30th IEEE International Symposium on High-Performance Computer Architecture (HPCA 2024), Edinburgh, Scotland","year":2024},{"abstract":"Improving cache performance is a lasting research\\ntopic. While utilizing data locality to enhance cache performance\\nbecomes more and more difficult, data access concurrency provides\\na new opportunity for cache performance optimization. In this\\nwork, we propose a novel concurrency-aware cache management\\nframework that outperforms state-of-the-art locality-only cache\\nmanagement schemes. First, we investigate the merit of data\\naccess concurrency and pinpoint that reducing the miss rate\\nmay not necessarily lead to better overall performance. Next, we\\nintroduce the pure miss contribution (PMC) metric, a lightweight\\nand versatile concurrency-aware indicator, to accurately measure\\nthe cost of each outstanding miss access by considering data\\nconcurrency. Then, we present CARE, a dynamic adjustable,\\nconcurrency-aware, low-overhead cache management framework\\nwith the help of the PMC metric. We evaluate CARE with\\nextensive experiments across different application domains and\\nshow significant performance gains with the consideration of data\\nconcurrency. In a 4-core system, CARE improves IPC by 10.3%\\nover LRU replacement. In 8 and 16-core systems where more\\nconcurrent data accesses exist, CARE outperforms LRU by 13.0%\\nand 17.1%, respectively.","authors":["X. Lu","R. Wang","X.-H. Sun"],"date":"February, 2023","doi":"10.1109/hpca56546.2023.10071125","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/lu2023care.bib","citation":"http://cs.iit.edu/~scs/assets/files/lu2023care.txt","pdf":"http://cs.iit.edu/~scs/assets/files/lu2023care.pdf","slides":"http://cs.iit.edu/~scs/assets/files/lu2023care-slides.pdf"},"month":2,"slug":"lu-2023-care-bc99","tags":["Optimization of Memory Architectures"],"title":"CARE: A Concurrency-Aware Enhanced Lightweight Cache Management Framework","type":"Conference","venue":"The 29th IEEE International Symposium on High-Performance Computer Architecture (HPCA-29), Montreal, QC, Canada, February 25 - March 01, 2023","year":2023},{"abstract":"With the surge of big data applications and the worsening of the memory-wall problem, the memory system,\\ninstead of the computing unit, becomes the commonly recognized major concern of computing. However, this \\"memory-\\ncentric\\" common understanding has a humble beginning. More than three decades ago, the memory-bounded speedup\\nmodel is the first model recognizing memory as the bound of computing and provided a general bound of speedup and a\\ncomputing-memory trade-off formulation. The memory-bounded model was well received even by then. It was immediate-\\nly introduced in several advanced computer architecture and parallel computing textbooks in the 1990\'s as a must-know\\nfor scalable computing. These include Prof. Kai Hwang\'s book \\"Scalable Parallel Computing\\" in which he introduced the\\nmemory-bounded speedup model as the Sun-Ni\'s Law, parallel with the Amdahl\'s Law and the Gustafson\'s Law. Through\\nthe years, the impacts of this model have grown far beyond parallel processing and into the fundamental of computing. In\\nthis article, we revisit the memory-bounded speedup model and discuss its progress and impacts in depth to make a unique\\ncontribution to this special issue, to stimulate new solutions for big data applications, and to promote data-centric think-\\ning and rethinking.","authors":["X.-H. Sun","X. Lu"],"date":"February, 2023","doi":"10.1007/s11390-022-2911-1","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/sun2023memory.bib","citation":"http://cs.iit.edu/~scs/assets/files/sun2023memory.txt","pdf":"http://cs.iit.edu/~scs/assets/files/sun2023memory.pdf"},"month":2,"slug":"sun-2023-memory-bounded-824c","tags":["Memory-Bounded Speedup","Scalable Computing","Memory-Wall","Data-Centric Design","Optimization of Memory Architectures"],"title":"The Memory-Bounded Speedup Model and Its Impacts in Computing","type":"Journal","venue":"Journal of Computer Science and Technology (JCST\'23), vol. 38, no. 1, February 2023","year":2023},{"abstract":"Memory system is critical to architecture design which can significantly impact application performance.\\nConcurrent Average Memory Access Time (C-AMAT) is a model for analyzing and optimizing memory\\nsystem performance using a recursive definition of the memory access latency along the memory hierarchy.\\nThe original C-AMAT model, however, does not provide the necessary granularity and flexibility for handling\\nmodern memory architectures with heterogeneous memory technologies and diverse system topology. We\\npropose to augment C-AMAT to take into consideration the idiosyncrasies of individual cache/memory\\ncomponents as well as their topological arrangement in the memory architecture design. Through trace-\\nbased simulation, we validate the augmented model and examine the memory system performance with\\ninsight unavailable using the original C-AMAT model.","authors":["H. Najafi","X. Lu","J. Liu","X.-H. Sun"],"date":"December, 2022","doi":"10.1109/wsc57314.2022.10015298","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/najafi2022generalized.bib","citation":"http://cs.iit.edu/~scs/assets/files/najafi2022generalized.txt","pdf":"http://cs.iit.edu/~scs/assets/files/najafi2022generalized.pdf"},"month":12,"slug":"najafi-2022-generalized-model-3e33","tags":["Hierarchical Memory System","C-AMAT","Optimization of Memory Architectures"],"title":"A Generalized Model For Modern Hierarchical Memory System","type":"Conference","venue":"The 2022 Winter Simulation Conference (WSC), Singapore, December 11-14, 2022","year":2022},{"abstract":"As the number of on-chip cores and application de-\\nmands increase, efficient management of shared cache resources\\nbecomes imperative. Cache partitioning techniques have been\\nstudied for decades to reduce interference between applications in\\na shared cache and provide performance and fairness guarantees.\\nHowever, there are few studies on how concurrent memory\\naccesses affect the effectiveness of partitioning. When concurrent\\nmemory requests exist, cache miss does not reflect concurrency\\noverlapping well. In this work, we first introduce pure misses\\nper kilo instructions (PMPKI), a metric that quantifies the\\ncache efficiency considering concurrent access activities. Then\\nwe propose Premier, a dynamically adaptive concurrency-aware\\ncache pseudo-partitioning framework. Premier provides insertion\\nand promotion policies based on PMPKI curves to achieve the\\nbenefits of cache partitioning. Finally, our evaluation of various\\nworkloads shows that Premier outperforms state-of-the-art cache\\npartitioning schemes in terms of performance and fairness.\\nIn an 8-core system, Premier achieves 15.45% higher system\\nperformance and 10.91% better fairness than the UCP scheme.","authors":["X. Lu","R. Wang","X.-H. Sun"],"date":"October, 2021","doi":"10.1109/iccd53106.2021.00068","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/lu2021premier.bib","citation":"http://cs.iit.edu/~scs/assets/files/lu2021premier.txt","pdf":"http://cs.iit.edu/~scs/assets/files/lu2021premier.pdf"},"month":10,"slug":"lu-2021-premier-10f5","tags":["Optimization of Memory Architectures"],"title":"Premier: A Concurrency-Aware Pseudo-Partitioning Framework for Shared Last-Level Cache","type":"Conference","venue":"The 2021 IEEE 39th International Conference on Computer Design (ICCD\'21), October 24 - 27, 2021","year":2021},{"abstract":"Processing-in-Memory (PIM) is considered a\\npromising solution to improve the performance of graph-\\ncomputing applications by minimizing the data movement be-\\ntween the host and memory. Which workload to offload and how\\nto offload it to PIM logic determine whether the PIM architecture\\nis well utilized. Offloading too much or too little workload\\nfrom the host processor to the PIM side could hurt overall\\nperformance. On the other hand, the offloading granularity\\nneeds to be representative without losing generality. In this\\npaper, we present CoPIM, a novel PIM workload offloading\\narchitecture that can dynamically determine which portion of the\\ngraph workload can benefit more from PIM-side computation.\\nCOPIM focuses on the loop code blocks of graph applications\\nand evaluates the necessity of offloading based on a concurrent\\nmemory access model. We also provide detailed architectural\\ndesigns to support the offloading. In this way, CoPIM reduces\\nthe size of offloading instructions and also improves the overall\\nperformance with less energy consumption. The experimental\\nresults show that compared with other state-of-the-art PIM\\nworkload offloading frameworks, CoPIM achieves a speedup\\nby the geometric mean of 19.5% and 11.4% than PEI and\\nGraphPIM, respectively. On the other hand, CoPIM also reduces\\nthe un-core energy consumption by 6.8% and 6.5% on average\\nover PEI and GraphPIM, respectively.","authors":["L. Yan","M. Zhang","R. Wang","X. Chen","X. Zou","X. Lu","Y. Han","X.-H. Sun"],"date":"July, 2021","doi":"10.1109/islped52811.2021.9502483","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/yan2021COPIM.bib","citation":"http://cs.iit.edu/~scs/assets/files/yan2021COPIM.txt","pdf":"http://cs.iit.edu/~scs/assets/files/yan2021COPIM.pdf"},"month":7,"slug":"yan-2021-copim-a500","tags":[],"title":"CoPIM: A Concurrency-aware PIM Workload Offloading Architecture for Graph Applications","type":"Conference","venue":"The 2021 ACM/IEEE International Symposium on Low Power Electronics and Design (ISLPED\'21), July 26, 2021","year":2021},{"abstract":"Prefetching techniques have been studied for\\ndecades. However, there are few studies on how concurrent\\nmemory accesses may affect prefetching effectiveness. When\\nthere are multiple concurrent memory requests, we can classify\\nthem into sub-classes by analyzing the overlapping relationship.\\nIn this work, we first propose pure prefetch coverage (PPC), a\\nnovel prefetching metric that can identify an accurate prefetch\\ncoverage under the concurrent memory access model. Then\\nwe propose APAC, an adaptive prefetch framework with PPC\\nmetric that can capture the dynamics of applications and adjust\\nthe prefetching aggressiveness. Our experimental results show\\nthat the PPC metric has a higher IPC correlation compared to\\nthe conventional prefetch coverage (PC) metric. For memory-\\nintensive single-thread benchmarks, APAC provides an average\\nperformance improvement by 17.3% and 5.9% compared to the\\nstate-of-the-art adaptive prefetch framework FDP and NST. In\\na multi-core system, APAC outperforms FDP and NST by 8.5%\\nand 5.0% IPC on average, respectively.","authors":["X. Lu","R. Wang","X.-H. Sun"],"date":"October, 2020","doi":"10.1109/iccd50377.2020.00048","links":{"bibtex":"http://cs.iit.edu/~scs/assets/files/lu2020apac.bib","citation":"http://cs.iit.edu/~scs/assets/files/lu2020apac.txt","pdf":"http://cs.iit.edu/~scs/assets/files/lu2020apac.pdf"},"month":10,"slug":"lu-2020-apac-66b5","tags":["Prefetch","Memory Performance Model","Concurrent Memory Access","Optimization of Memory Architectures"],"title":"APAC: An Accurate and Adaptive Prefetch Framework with Concurrent Memory Access Analysis","type":"Conference","venue":"The 38th IEEE International Conference on Computer Design (ICCD\'20), October 18 - 21, 2020","year":2020}]')}}]);