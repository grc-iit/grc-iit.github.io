"use strict";(self.webpackChunkgrc=self.webpackChunkgrc||[]).push([[11824],{94825:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>r,toc:()=>m});const r=JSON.parse('{"type":"mdx","permalink":"/research/projects/optmem","source":"@site/src/pages/research/projects/optmem.mdx","title":"Optimization of Memory Architectures: A Foundation Approach","description":"Background","frontMatter":{"title":"Optimization of Memory Architectures: A Foundation Approach"},"unlisted":false}');var i=t(74848),a=t(28453),s=t(18845),c=t(88017);const o={title:"Optimization of Memory Architectures: A Foundation Approach"},l="Optimization of Memory Architectures: A Foundation Approach",d={},m=[{value:"Background",id:"background",level:2},{value:"Project Contributions",id:"project-contributions",level:2},{value:"1. Unified Mathematical Framework for Memory Access",id:"1-unified-mathematical-framework-for-memory-access",level:3},{value:"2. Concurrency-Aware Performance Optimization Frameworks",id:"2-concurrency-aware-performance-optimization-frameworks",level:3},{value:"Project Significance",id:"project-significance",level:2},{value:"Publications",id:"publications",level:2},{value:"Sponsor",id:"sponsor",level:2}];function h(e){const n={h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"optimization-of-memory-architectures-a-foundation-approach",children:"Optimization of Memory Architectures: A Foundation Approach"})}),"\n",(0,i.jsx)(s.A,{projectId:"optmem"}),"\n",(0,i.jsx)(n.h2,{id:"background",children:"Background"}),"\n",(0,i.jsx)(n.p,{children:'The "memory wall" problem is a significant performance bottleneck in modern computer architectures,\ndriven by an ever-widening disparity between CPU and memory speeds. Despite advancements over the\npast three decades\u2014including multi-core and many-core designs, deep pipelining, and innovative\ncaching techniques (such as multi-port, multi-banked, pipelined, and non-blocking caches)\u2014the\n"memory wall" persists. If anything, the problem has intensified with the rise of data-intensive\napplications, which place additional demands on memory systems.'}),"\n",(0,i.jsx)(n.p,{children:"Concurrent Average Memory Access Time (C-AMAT) is a memory performance model that extends the\ntraditional Average Memory Access Time (AMAT) to address the complexities of modern systems,\nwhere concurrent memory accesses are prevalent. Unlike AMAT, which assumes sequential memory\naccess, C-AMAT integrates both data concurrency and locality into a single unified metric.\nThis metric applies recursively across all layers of the memory hierarchy, making C-AMAT an\ninvaluable tool for accurately modeling and optimizing memory systems in contemporary architectures."}),"\n",(0,i.jsx)(n.p,{children:"This project aims to develop advanced memory-architecture performance-modeling and optimization\nframeworks that effectively capture the combined effects of data locality, data concurrency,\nand data access overlap. This work provides a foundation for future architecture designs that\nbetter balance CPU and memory capabilities, paving the way for more efficient and scalable\nsystems in the data-driven era."}),"\n",(0,i.jsx)(n.h2,{id:"project-contributions",children:"Project Contributions"}),"\n",(0,i.jsx)(n.h3,{id:"1-unified-mathematical-framework-for-memory-access",children:"1. Unified Mathematical Framework for Memory Access"}),"\n",(0,i.jsx)(n.p,{children:"A coherent mathematical framework was developed for a memory-centric view of data accesses, which\nenables a recursive definition of memory access latency and concurrency along the memory hierarchy,\nproviding clearer insights into memory performance. The Concurrent Average Memory Access Time (C-AMAT)\nmodel was expanded to cover more general situations in hierarchical memory systems, accounting for\nsplitting and merging at specific cache memory devices. Additionally, partitioning techniques were\nrevisited to account for concurrent data access."}),"\n",(0,i.jsx)("center",{children:(0,i.jsx)("img",{src:t(92943).A,width:"800",alt:"A Memory System Viewed as a Multi-tree"})}),"\n",(0,i.jsx)("center",{children:(0,i.jsx)("p",{children:(0,i.jsx)("strong",{children:"A Memory System Viewed as a Multi-tree."})})}),"\n",(0,i.jsx)(n.h3,{id:"2-concurrency-aware-performance-optimization-frameworks",children:"2. Concurrency-Aware Performance Optimization Frameworks"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"APAC Prefetch Framework"}),": APAC is an adaptive prefetch framework that adjusts prefetch aggressiveness\nbased on concurrent memory access patterns, optimizing both prefetch accuracy and coverage. In both\nsingle-core and multi-core environments, APAC demonstrates substantial performance improvements, with\nan average 17.3% IPC gain over state-of-the-art adaptive frameworks."]}),"\n"]}),"\n",(0,i.jsx)("center",{children:(0,i.jsx)("img",{src:t(11445).A,width:"800",alt:"Adjust Prefetch Aggressiveness with Runtime Metrics"})}),"\n",(0,i.jsx)("center",{children:(0,i.jsx)("p",{children:(0,i.jsx)("strong",{children:"Adjust Prefetch Aggressiveness with Runtime Metrics."})})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Premier Cache Partitioning Framework"}),": Premier is a concurrency-aware cache\npartitioning framework that leverages the Pure Misses Per Kilo Instructions (PMPKI)\nmetric to dynamically allocate cache capacity and mitigate interference. Premier\noffers adaptive insertion and dynamic capacity allocation to maximize cache efficiency.\nPremier outperforms traditional partitioning schemes, achieving a 15.45% performance\nimprovement and a 10.91% fairness increase in 8-core systems, demonstrating its strength\nin managing shared LLC resources effectively."]}),"\n"]}),"\n",(0,i.jsx)("center",{children:(0,i.jsx)("img",{src:t(9780).A,width:"800",alt:"PMPKI and CPI for Varying Cache Sizes in SPEC Workloads"})}),"\n",(0,i.jsx)("center",{children:(0,i.jsx)("p",{children:(0,i.jsx)("strong",{children:"PMPKI and CPI for Varying Cache Sizes in SPEC Workloads."})})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"CARE Concurrency-Aware Cache Management"}),": CARE introduces an innovative approach that goes beyond\ntraditional locality-based management, utilizing the Pure Miss Contribution (PMC) metric to prioritize\ncache replacements. CARE dynamically adjusts cache management decisions, aligning cache behavior with\nvarying workload demands for enhanced efficiency. CARE achieves substantial improvements over traditional\nLRU policies, with an average 10.3% IPC gain in 4-core systems and up to 17.1% IPC improvement in\n16-core systems. These results demonstrate CARE's scalability and effectiveness in high-concurrency environments."]}),"\n"]}),"\n",(0,i.jsx)("center",{children:(0,i.jsx)("img",{src:t(64425).A,width:"800",alt:"The Overview of the CARE Design"})}),"\n",(0,i.jsx)("center",{children:(0,i.jsx)("p",{children:(0,i.jsx)("strong",{children:"The Overview of the CARE Design."})})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"CHROME Holistic Cache Management"}),": CHROME is an innovative cache management framework that leverages\nconcurrency-aware, online reinforcement learning to optimize cache performance in dynamic environments.\nBy continuously interacting with the processor and memory system, CHROME learns and adapts its cache\nmanagement policy in real-time. This online reinforcement learning approach enables CHROME to perform\neffectively across diverse system configurations and fluctuating workloads. CHROME makes bypassing and\nreplacement decisions based on multiple program features and system-level feedback, considering\nconcurrency to enhance overall cache efficiency. Extensive evaluations show that CHROME consistently\noutperforms traditional cache management schemes, achieving a 13.7% performance improvement in 16-core\nsystems, proving its potential for improving cache performance in data-intensive, scalable computing systems."]}),"\n"]}),"\n",(0,i.jsx)("center",{children:(0,i.jsx)("img",{src:t(5592).A,width:"800",alt:"The Overview of the CHROME Design"})}),"\n",(0,i.jsx)("center",{children:(0,i.jsx)("p",{children:(0,i.jsx)("strong",{children:"The Overview of the CHROME Design."})})}),"\n",(0,i.jsx)(n.h2,{id:"project-significance",children:"Project Significance"}),"\n",(0,i.jsx)(n.p,{children:"The Optimization of Memory Architectures project addresses critical challenges in modern memory architecture,\nproviding both theoretical and practical advances in memory performance modeling. By considering data locality\nand concurrency, this research establishes a foundation for scalable, high-performance computing architectures.\nThese advancements equip future systems to efficiently support the most demanding data-intensive applications,\npaving the way for breakthroughs in computer architecture, high-performance computing, and beyond."}),"\n",(0,i.jsx)(n.h2,{id:"publications",children:"Publications"}),"\n",(0,i.jsx)(c.A,{tag:"Optimization of Memory Architectures"}),"\n",(0,i.jsx)(n.h2,{id:"sponsor",children:"Sponsor"}),"\n",(0,i.jsx)("p",{children:(0,i.jsx)("img",{src:t(40897).A,width:"100"})}),"\n",(0,i.jsx)(n.p,{children:"This research is supported by the National Science Foundation under Grant CCF-2008907."})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}},18845:(e,n,t)=>{t.d(n,{A:()=>d});t(96540);var r=t(34164),i=t(46784),a=t(66188),s=t(71429),c=t(66588);const o={badgeDarker:"badgeDarker_Lm_2"};var l=t(74848);function d({addMargin:e=!0,projectId:n}){const{projects:t}=(0,c.P_)("grc-plugin-projects"),d=(0,s.G)(t,n);if(!d)return null;const{isOurs:m=!1,sourceLink:h,tutorialLink:u,type:g}=d,p="funded"===g;return p||m||h||u?(0,l.jsxs)("div",{className:(0,r.A)(e&&"margin-bottom--md"),children:[m&&(0,l.jsx)("span",{className:"badge badge--primary margin-right--xs",children:"GRC-led"}),p&&(0,l.jsx)("span",{className:"badge badge--success margin-right--xs",children:"Funded"}),void 0!==h&&(0,l.jsxs)("a",{className:(0,r.A)("badge badge--secondary margin-right--xs",o.badgeDarker),href:h,rel:"noreferrer",style:{color:"var(--ifm-color-black) !important"},target:"_blank",children:["Open Source",(0,l.jsx)(i.g,{className:"margin-left--xs",icon:a.Ju_,size:"sm",style:{color:"var(--ifm-color-black)"}})]}),void 0!==u&&(0,l.jsxs)("a",{className:"badge badge--danger margin-right--xs",href:u,rel:"noreferrer",style:{backgroundColor:"var(--ifm-color-warning-lightest) !important",borderColor:"var(--ifm-color-warning-lightest) !important",color:"var(--ifm-color-black) !important"},target:"_blank",children:["Tutorial",(0,l.jsx)(i.g,{className:"margin-left--xs",icon:a.Ju_,size:"sm",style:{color:"var(--ifm-color-black)"}})]})]}):null}},88017:(e,n,t)=>{t.d(n,{A:()=>o});var r=t(68626),i=(t(96540),t(71429)),a=t(66588),s=t(74848);const c=(e,n)=>e.filter((e=>e.tags.includes(n)));function o({projectId:e,tag:n}){const{publications:t}=(0,a.P_)("grc-plugin-publications"),{projects:o}=(0,a.P_)("grc-plugin-projects");let l=[];if(e){const n=(0,i.G)(o,e);n&&(l=c(t,n.name))}else{if(!n)throw new Error("Either projectId or tag must be provided");l=c(t,n)}return(0,s.jsx)(r.A,{data:l,isFooterVisible:!1,isSearchInputVisible:!1,isTagsColumnVisible:!1})}},68626:(e,n,t)=>{t.d(n,{A:()=>x});var r=t(96540),i=t(74848);function a({value:e,onChange:n,debounce:t=500,...a}){const[s,c]=(0,r.useState)(e);return(0,r.useEffect)((()=>{c(e)}),[e]),(0,r.useEffect)((()=>{const e=setTimeout((()=>{n(s)}),t);return()=>clearTimeout(e)}),[s]),(0,i.jsx)("input",{...a,value:s,onChange:e=>c(e.target.value)})}var s=t(34164),c=t(33888),o=t(1530),l=t(46784),d=t(66188);const m={tag:"tag_ZGkR"};function h({children:e,className:n,maxWidth:t=150,...r}){return(0,i.jsx)("span",{...r,className:(0,s.A)("badge",m.tag,n),style:{maxWidth:t},children:e})}const u={author:"author_Z_S7",type:"type_KtxA",filters:"filters_WQpP",filterId:"filterId_HnvW",filterRemove:"filterRemove__W9Y",input:"input_BuGr",inputContainer:"inputContainer_TuAo",link:"link_xoWC",table:"table_sT_R",tags:"tags_FOFR",tag:"tag_hfRy"},g=", ";function p({columnFilters:e,getColumn:n}){const t=(0,r.useCallback)(((e,t)=>{const r=n(e),i=r.getFilterValue();if(i instanceof Array){const e=i.filter((e=>e!==t));0===e.length?r.setFilterValue(void 0):r.setFilterValue(e)}else r.setFilterValue(void 0)}),[n]);return 0===e.length?null:(e.sort(((e,n)=>e.id.localeCompare(n.id))),(0,i.jsx)("div",{className:(0,s.A)("margin-bottom--md",u.filters),children:e.map((e=>(0,i.jsxs)("span",{className:"badge badge--primary margin-right--sm",children:[(0,i.jsx)("span",{className:u.filterId,children:`${e.id}:`}),e.value instanceof Array?e.value.map((n=>(0,i.jsxs)(r.Fragment,{children:[(0,i.jsx)("span",{className:"margin-left--sm",children:`${n} `}),(0,i.jsx)(l.g,{className:u.filterRemove,icon:d.Jyw,onClick:()=>t(e.id,n)})]},n))):String(e.value)]},e.id)))}))}function f({column:e,data:n}){const t=(0,r.useCallback)((n=>{const t=e.getFilterValue()||[];t.includes(n)||e.setFilterValue([...t,n])}),[e,n]);return n.map(((e,a)=>(0,i.jsxs)(r.Fragment,{children:[(0,i.jsx)("span",{className:u.author,onClick:()=>t(e),children:e}),a!==n.length-1&&", ",a!==n.length-1&&(0,i.jsx)("br",{})]},`author-${a}`)))}function y({data:e}){return 0===Object.keys(e).length?"TBA":(0,i.jsx)("div",{className:u.links,children:Object.entries(e).map((([n,t],a)=>(0,i.jsxs)(r.Fragment,{children:[(0,i.jsx)("a",{className:u.link,href:t,target:"_blank",children:"pdf"===n?"PDF":"bibtex"===n?"BibTeX":n}),a!==Object.keys(e).length-1&&", ",a!==Object.keys(e).length-1&&(0,i.jsx)("br",{})]},`link-${a}`)))})}function v({column:e,data:n}){const t=(0,r.useCallback)((n=>{const t=e.getFilterValue()||[];t.includes(n)||e.setFilterValue([...t,n])}),[e,n]);return 0===n.length||""===n[0]?null:(0,i.jsx)("div",{className:u.tags,children:n.map(((e,n)=>(0,i.jsx)(h,{className:(0,s.A)("margin-bottom--xs",u.tag),onClick:()=>t(e),title:e,children:e},`tag-${n}`)))})}function j({column:e,data:n}){const t=(0,r.useCallback)((n=>{const t=e.getFilterValue()||[];t.includes(n)||e.setFilterValue([...t,n])}),[e,n]);return(0,i.jsx)("span",{className:u.type,onClick:()=>t(n),children:n})}function x({data:e,isFooterVisible:n=!0,isSearchInputVisible:t=!0,isTagsColumnVisible:s=!0}){const[l,d]=(0,r.useState)(""),[m,h]=(0,r.useState)([]),x=(0,r.useMemo)((()=>{const e=(0,c.FB)();return[e.accessor((e=>e.authors.join(g)),{cell:e=>(0,i.jsx)(f,{column:e.column,data:e.getValue().split(g)}),enableSorting:!1,filterFn:(e,n,t)=>{const r=e.getValue(n).split(g);return t.every((e=>r.includes(e)))},header:"Authors",id:"authors"}),e.accessor("title",{cell:e=>(0,i.jsx)("a",{href:`/publications/${e.row.original.slug}`,children:e.getValue()}),header:"Title",id:"title"}),e.accessor("venue",{cell:e=>e.getValue(),header:"Venue",id:"venue"}),e.accessor("type",{cell:e=>(0,i.jsx)(j,{column:e.column,data:e.getValue()}),filterFn:(e,n,t)=>{const r=e.getValue(n);return t.includes(r)},header:"Type",id:"type"}),e.accessor("date",{cell:e=>e.getValue(),header:"Date",id:"date"}),s&&e.accessor((e=>e.tags.join(g)),{cell:e=>(0,i.jsx)(v,{column:e.column,data:e.getValue().split(g)}),enableSorting:!1,filterFn:(e,n,t)=>{const r=e.getValue(n).split(g);return t.every((e=>r.includes(e)))},header:"Tags",id:"tags"}),e.accessor("links",{cell:e=>(0,i.jsx)(y,{data:e.getValue()}),header:"Links",id:"links"})].filter(Boolean)}),[]),b=(0,o.N4)({data:e,debugAll:!1,columns:x,getCoreRowModel:(0,c.HT)(),getFilteredRowModel:(0,c.hM)(),getSortedRowModel:(0,c.h5)(),onGlobalFilterChange:d,onSortingChange:h,state:{globalFilter:l,sorting:m}});return(0,i.jsxs)("div",{children:[t&&(0,i.jsx)("div",{className:u.inputContainer,children:(0,i.jsx)(a,{className:u.input,value:l??"",onChange:e=>d(String(e)),placeholder:"Seach publications"})}),(0,i.jsx)(p,{columnFilters:b.getState().columnFilters,getColumn:b.getColumn}),(0,i.jsxs)("table",{className:u.table,children:[(0,i.jsx)("thead",{children:b.getHeaderGroups().map((e=>(0,i.jsx)("tr",{children:e.headers.map((e=>(0,i.jsx)("th",{scope:"col",children:(0,i.jsxs)("div",{onClick:e.column.getToggleSortingHandler(),style:{cursor:e.column.getCanSort()?"pointer":"default"},children:[(0,o.Kv)(e.column.columnDef.header,e.getContext()),{asc:" \ud83d\udd3c",desc:" \ud83d\udd3d"}[e.column.getIsSorted()]??null]})},e.id)))},e.id)))}),(0,i.jsx)("tbody",{children:b.getRowModel().rows.map((e=>(0,i.jsx)("tr",{children:e.getVisibleCells().map((e=>(0,i.jsx)("td",{children:(0,o.Kv)(e.column.columnDef.cell,e.getContext())},e.id)))},e.id)))})]}),n&&(0,i.jsx)("div",{children:(0,i.jsxs)("span",{children:["Showing ",b.getRowModel().rows.length," of ",e.length," ","publications"]})})]})}},71429:(e,n,t)=>{t.d(n,{G:()=>r});const r=(e,n)=>e.find((e=>e.id===n))},40897:(e,n,t)=>{t.d(n,{A:()=>r});const r=t.p+"assets/images/nsf-fb7efe9286a9b499c5907d82af3e70fd.png"},11445:(e,n,t)=>{t.d(n,{A:()=>r});const r=t.p+"assets/images/APAC-8b950d200d8843c90438440260be58ab.png"},64425:(e,n,t)=>{t.d(n,{A:()=>r});const r=t.p+"assets/images/CARE-58fb692c8f49d103ffd76aef0e46eb6c.png"},5592:(e,n,t)=>{t.d(n,{A:()=>r});const r=t.p+"assets/images/CHROME-12ae4164e053c949c5e9995eee30b817.png"},9780:(e,n,t)=>{t.d(n,{A:()=>r});const r=t.p+"assets/images/Premier-5bbee7795f0f4a2747be7cb5ff5f3e24.png"},92943:(e,n,t)=>{t.d(n,{A:()=>r});const r=t.p+"assets/images/model-34f6de2bbd031d08cad344c5ebbee5e1.png"},28453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>c});var r=t(96540);const i={},a=r.createContext(i);function s(e){const n=r.useContext(a);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);