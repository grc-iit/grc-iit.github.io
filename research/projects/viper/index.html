<!doctype html>
<html lang="en" dir="ltr" class="mdx-wrapper mdx-page plugin-pages plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.1">
<title data-rh="true">Viper: A High-Performance I/O Framework for Transparently Updating, Storing, and Transferring Deep Neural Network Models | Gnosis Research Center</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://grc.iit.edu/img/social-card.png"><meta data-rh="true" name="twitter:image" content="https://grc.iit.edu/img/social-card.png"><meta data-rh="true" property="og:url" content="https://grc.iit.edu/research/projects/viper"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" name="keywords" content="high-performance computing (hpc), memory and storage systems, scalable software systems, data management, data-centric systems, research center, scalable system software, technology research, computational research, innovative technology, scientific research"><meta data-rh="true" property="og:title" content="Viper: A High-Performance I/O Framework for Transparently Updating, Storing, and Transferring Deep Neural Network Models | Gnosis Research Center"><meta data-rh="true" name="description" content="Overview"><meta data-rh="true" property="og:description" content="Overview"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://grc.iit.edu/research/projects/viper"><link data-rh="true" rel="alternate" href="https://grc.iit.edu/research/projects/viper" hreflang="en"><link data-rh="true" rel="alternate" href="https://grc.iit.edu/research/projects/viper" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Gnosis Research Center RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Gnosis Research Center Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-9SXDPLSLB2"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-9SXDPLSLB2",{})</script>



<link rel="alternate" type="application/rss+xml" href="/newsletter/rss.xml" title="Gnosis Research Center RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/newsletter/atom.xml" title="Gnosis Research Center Atom Feed"><link rel="stylesheet" href="/assets/css/styles.5529de29.css">
<script src="/assets/js/runtime~main.3d4240b4.js" defer="defer"></script>
<script src="/assets/js/main.cb7021a7.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="light";var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top navbar--dark"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.png" alt="Gnosis Research Center" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.png" alt="Gnosis Research Center" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Gnosis Research Center</b></a><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Gnosis</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/about">About</a></li><li><a class="dropdown__link" href="/members">Members</a></li><li><a class="dropdown__link" href="/network">Network</a></li><li><a class="dropdown__link" href="/contact">Contact</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Research</a><ul class="dropdown__menu"><li><a aria-current="page" class="dropdown__link dropdown__link--active" href="/research/projects">Projects</a></li><li><a class="dropdown__link" href="/research/patents">Patents</a></li><li><a class="dropdown__link" href="/resources/hardware-overview">Hardware Overview</a></li></ul></div><a class="navbar__item navbar__link" href="/news">News</a><a class="navbar__item navbar__link" href="/publications">Publications</a><a class="navbar__item navbar__link" href="/opportunities">Opportunities</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/docs/index">Tutorials</a><a href="https://github.com/grc-iit" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS darkNavbarColorModeToggle_X3D1" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><main class="container container--fluid margin-vert--lg"><div class="row mdxPageWrapper_j9I6"><div class="col col--8"><article><p><img src="/assets/images/logo-95bd4156d3a78ae2165cd6b0cd2d41b8.png" width="200"></p>
<header><h1>Viper: A High-Performance I/O Framework for Transparently Updating, Storing, and Transferring Deep Neural Network Models</h1></header>
<div class="margin-bottom--md"><span class="badge badge--primary margin-right--xs">GRC-led</span><span class="badge badge--success margin-right--xs">Funded</span></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="overview">Overview<a href="#overview" class="hash-link" aria-label="Direct link to Overview" title="Direct link to Overview">​</a></h2>
<p>Scientific workflows are increasingly using Deep Learning (DL), requiring a Deep Neural Network (DNN) model to be
trained and used for inferences at the same time. A common approach is for the training server (producer) and the
inference server (consumer) to use separate model replicas that are kept synchronized. This setup, however, creates
two major challenges:</p>
<ol>
<li>
<p><strong>Trade-off between Updates and Performance</strong>: A frequent model update schedule can improve inference quality because
the consumer uses a more up-to-date model, but it can also slow down the training process due to the overhead of
creating and transferring checkpoints. Conversely, infrequent updates may lead to less precise inference results.</p>
</li>
<li>
<p><strong>Inefficient Model Transfer</strong>: Traditional methods for sharing models between the producer and consumer often rely on
an intermediate staging area (e.g., PFS), causing significant delays due to I/O bottlenecks and the use of fixed-interval
polling by the consumer to detect new models.</p>
</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-contributions">Key Contributions<a href="#key-contributions" class="hash-link" aria-label="Direct link to Key Contributions" title="Direct link to Key Contributions">​</a></h2>
<p><strong>Viper</strong> is high-performance I/O framework designed to both determine a near-optimal checkpoint schedule and accelerate the delivery
of model updates. It is built on two core innovations:</p>
<ol>
<li>
<p><strong>Inference Performance Predictor (IPP)</strong>: Identifies a near-optimal checkpoint schedule to effectively
balance the trade-off between training slowdown and inference quality improvement.</p>
</li>
<li>
<p><strong>Memory-first Model Transfer Engine</strong>: Accelerates model delivery by using direct
memory-to-memory communication, bypassing slower storage like PFS. It prioritizes GPU-to-GPU memory transfer when
available, falling back to host-to-host RDMA transfer if needed. This asynchronous engine, combined with a push-based
notification module, ensures that consumers are promptly notified of new model updates without relying on polling.</p>
</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="background">Background<a href="#background" class="hash-link" aria-label="Direct link to Background" title="Direct link to Background">​</a></h2>
<p>In traditional DL workflow, producer (Scientific AI Application) typically trains a DNN model offline with a fixed set of input data
and then persists the trained model to a model repository for future use, while consumer (Inference Serving System) will load the pre-trained
DNN model from the model repository and offers online inference queries for end-user applications.</p>
<p>However, this offline training is not an ideal choice in two scenarios:</p>
<ul>
<li><strong>Scenario 1:</strong> Modern scientific DL workflows often operate in dynamic environments where new data is constantly changing and accumulating over time.<!-- -->
<ul>
<li>To adapt to data changes, continuous learning is utilized to continuously (re)-train a DNN model by using some online techniques.</li>
<li>Continuous learning implies the continuous deployment of the DNN model to keep the model up-to-date</li>
</ul>
</li>
<li><strong>Scenario 2:</strong> The consumer may have a limited time window for inferences, it may need to start inferencing after the warmup phase in model training on the producer side<!-- -->
<ul>
<li>Producer continues training the model while the consumer conducts inferences</li>
<li>This requires the intermediate DNN models to be consistently delivered from the producer to the consumer during training to achieve high inference performance</li>
</ul>
</li>
</ul>
<center><p><img src="/assets/images/scenario2-3f2e0ace5443aaec287e4fd1dd06be5c.png" width="600"></p></center>
<p>Both scenarios <strong>increase the model update frequency</strong> between producers and consumers.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="motivation">Motivation<a href="#motivation" class="hash-link" aria-label="Direct link to Motivation" title="Direct link to Motivation">​</a></h2>
<center><p><img src="/assets/images/viper_motivation-adb60fce2603c4cdc64d2c5c67b6f5fc.png" width="600"></p></center>
<ol>
<li>Model update frequency <strong>affects both training and inference performance</strong>, since a model update operation involves both model checkpointing and model data delivery.E.g.,<!-- -->
<ul>
<li>Frequent model updates can enhance inference performance but may slow down training</li>
<li>Infrequent model updates may pose less overhead on training but may degrade the overall inference model accuracy</li>
</ul>
</li>
<li>Currently, Scientific AI Applications and Inference Serving Systems communicate through a model repository (e.g., PFS), as depicted in Figure (a). This communication method may result in:<!-- -->
<ul>
<li><strong>High model update latency</strong> due to the I/O bottlenecks caused by concurrent, uncoordinated, small I/O accesses to PFS</li>
<li><strong>High model discovery High model discovery latency on consumers</strong> due to the static fixed-interval pull-based (e.g., polling) detection mechanism</li>
</ul>
</li>
</ol>
<p>Thus, there is a need to 1) <strong>balance the trade-off</strong> between training and inference performance; 2) <strong>accelerate model data discovery and delivery</strong> between producers and consumers (Figure b).</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="approach">Approach<a href="#approach" class="hash-link" aria-label="Direct link to Approach" title="Direct link to Approach">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="vipers-high-level-architecture">Viper&#x27;s High-level Architecture<a href="#vipers-high-level-architecture" class="hash-link" aria-label="Direct link to Viper&#x27;s High-level Architecture" title="Direct link to Viper&#x27;s High-level Architecture">​</a></h3>
<center><p><img src="/assets/images/high_level_design-fa5f7f1ffcecd002a2449cbe17b93857.png" width="600"></p></center>
<p>Viper is a high-performance I/O framework to accelerate DNN models exchange between
producers and consumers. It aims to:</p>
<ol>
<li><strong>Balance the trade-off between training runtime and inference performance</strong>
<ul>
<li>Viper builds an <strong>intelligent inference performance predictor</strong> to achieve this object<!-- -->
<ul>
<li>Can <strong>decide an optimal model checkpoint schedule</strong> between producers and consumers</li>
<li>Supporting two different algorithms for finding the optimal checkpoint schedule</li>
</ul>
</li>
</ul>
</li>
<li><strong>Accelerate model data transfer</strong>
<ul>
<li>Viper creates a <strong>novel cache-aware data transfer engine</strong> to speedup model update between producers and consumers<!-- -->
<ul>
<li>Creating a direct data exchange channel for model delivery and utilizes. E.g., the direct GPU-to-GPU or RAM-to-RAM data transfer strategy</li>
<li>Utilizing a lightweight publish-subscribe notification mechanism to promptly inform the consumer of the model changes.</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="accelerate-model-data-transfer">Accelerate Model Data Transfer<a href="#accelerate-model-data-transfer" class="hash-link" aria-label="Direct link to Accelerate Model Data Transfer" title="Direct link to Accelerate Model Data Transfer">​</a></h3>
<center><p><img src="/assets/images/data_transfer-bd621a0c8a42b3401593200c131b1c32.png" width="400"></p></center>
<p>During training, DNN models can be cached on multiple alternative locations (e.g., GPU memory, Host memory, and PFS)</p>
<ol>
<li>Asynchronous memory-first engine<!-- -->
<ul>
<li>Utilize the cached models on different locations to accelerate data movement between producer and consumer</li>
<li>Creates a direct communication channel to transfer the model data<!-- -->
<ul>
<li>Direct GPU-to-GPU memory and host-to-host memory data transfer strategy</li>
<li>Implemented based on MPI library</li>
</ul>
</li>
</ul>
</li>
<li><strong>A lightweight publish-subscribe notification module</strong> to proactively inform consumers of model updates instead
of passively periodic queries</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="experiment-results">Experiment Results<a href="#experiment-results" class="hash-link" aria-label="Direct link to Experiment Results" title="Direct link to Experiment Results">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="end-to-end-model-update-latency">End-to-end Model Update Latency<a href="#end-to-end-model-update-latency" class="hash-link" aria-label="Direct link to End-to-end Model Update Latency" title="Direct link to End-to-end Model Update Latency">​</a></h3>
<center><div style="display:flex;flex-direction:column;align-items:flex-start;gap:12px"><div style="display:flex;gap:16px"><img src="/assets/images/latency_nt3_600-37ecde0b110ba468f4ef947a77e9e49a.png" width="250"><img src="/assets/images/latency_tc1_4600-8d0db1038c9b1e91ac4b6143ac8eb0d6.png" width="250"><img src="/assets/images/latency_ptychoNN-1a7c723c9e8a4cd6f1819aa8e36225fd.png" width="250"></div></div></center>
<p><strong>Goal:</strong> showcase the end-to-end model update latency across different model transfer strategies</p>
<p><strong>Observations:</strong></p>
<ol>
<li>Both GPU-to-GPU and Host-to-Host memory strategies achieve better performance than PFS<!-- -->
<ul>
<li>GPU-to-GPU outperforms the baseline by 12x for NT3, 9x for TC1, and 15x for PtychoNN using Viper-Async approach</li>
<li>Host-to-Host using Viper-Async approach is at least 3x better than baseline</li>
<li>This is attributed to high I/O bandwidth of the fast memory tiers and the high-speed network</li>
</ul>
</li>
<li>Viper-PFS approach is also ~1.2x faster than baseline since it only writes model weights and closely related metadata into the file</li>
<li>Viper-Async is slower than Viper-sync because it uses a separate thread for data transfer to reduce training interruption, requiring an extra data copy</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="inference-performance-predictor">Inference Performance Predictor<a href="#inference-performance-predictor" class="hash-link" aria-label="Direct link to Inference Performance Predictor" title="Direct link to Inference Performance Predictor">​</a></h3>
<center><div style="display:flex;justify-content:space-between"><div style="display:flex;gap:16px"><img src="/assets/images/predictor_nt3-14d3b0ea2b416cecde04ddd25ac44fe4.png" width="250"><img src="/assets/images/predictor_TC1-b192a364e1198a3f787a9ab59492c000.png" width="250"><img src="/assets/images/predictor_PtychoNN-8312de2f0659d6cc11de7ce312c370d9.png" width="250"></div></div></center>
<p><strong>Goal:</strong> showcase the checkpoint schedule identified by the Inference Performance Predictor (IPP) can achieve lower CIL compared to baseline</p>
<p><strong>Observations:</strong></p>
<ol>
<li>Both fixed-interval and adaptive-interval checkpoint schedule can achieve better CIL compared with the baseline (i.e., epoch-boundary checkpoint schedule)</li>
<li>Adaptive-interval checkpoint schedule is better than fixed-interval approach<!-- -->
<ul>
<li>For NT3 model, adaptive-interval schedule reduces the CIL from 3.8k to 3.0k</li>
<li>For TC1 model,  adaptive-interval schedule reduces the CIL from the 32.8k to 30.4k</li>
<li>For PtychoNN model, adaptive-interval schedule reduce the CIL from the baseline of 66.2k to 4.0k</li>
</ul>
</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="members">Members<a href="#members" class="hash-link" aria-label="Direct link to Members" title="Direct link to Members">​</a></h2>
<ul>
<li>Jie Ye, Illinois Institute of Technology</li>
<li>Jaime Cernuda, Illinois Institute of Technology</li>
<li>Bogdan Nicolae, Argonne National Laboratory</li>
<li>Anthony Kougkas, Illinois Institute of Technology</li>
<li>Xian-He Sun, Illinois Institute of Technology</li>
</ul></article></div><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#overview" class="table-of-contents__link toc-highlight">Overview</a></li><li><a href="#key-contributions" class="table-of-contents__link toc-highlight">Key Contributions</a></li><li><a href="#background" class="table-of-contents__link toc-highlight">Background</a></li><li><a href="#motivation" class="table-of-contents__link toc-highlight">Motivation</a></li><li><a href="#approach" class="table-of-contents__link toc-highlight">Approach</a><ul><li><a href="#vipers-high-level-architecture" class="table-of-contents__link toc-highlight">Viper&#39;s High-level Architecture</a></li><li><a href="#accelerate-model-data-transfer" class="table-of-contents__link toc-highlight">Accelerate Model Data Transfer</a></li></ul></li><li><a href="#experiment-results" class="table-of-contents__link toc-highlight">Experiment Results</a><ul><li><a href="#end-to-end-model-update-latency" class="table-of-contents__link toc-highlight">End-to-end Model Update Latency</a></li><li><a href="#inference-performance-predictor" class="table-of-contents__link toc-highlight">Inference Performance Predictor</a></li></ul></li><li><a href="#members" class="table-of-contents__link toc-highlight">Members</a></li></ul></div></div></div></main></div><footer class="theme-layout-footer footer"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Gnosis Research Center</div><ul class="footer__items clean-list"><li class="footer__item">
                  <p>
                    Stuart Building <br>
                    Room 112i and 010 <br>
                    10 W. 31st Street <br>
                    Chicago, Illinois 60616
                  </p>
                </li><li class="footer__item">
                  <p>
                    Email: <a href="mailto:grc@illinoistech.edu" target="_blank" rel="noopener noreferrer">grc@illinoistech.edu</a> <br>
                    Phone: <a href="tel:3125676885" target="_blank" rel="noopener noreferrer">+1 312 567 6885</a>
                  </p>
                </li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Featured Projects</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/research/projects/iowarp">IOWarp</a></li><li class="footer__item"><a class="footer__link-item" href="/research/projects/chronolog">ChronoLog</a></li><li class="footer__item"><a class="footer__link-item" href="/research/projects/labios">LABIOS</a></li><li class="footer__item"><a class="footer__link-item" href="/research/projects/coeus">Coeus</a></li><li class="footer__item"><a class="footer__link-item" href="/research/projects/hermes">Hermes</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Tutorials</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/category/linux-introduction">Linux Introduction</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/category/installing-hpc-software">Installing HPC Software</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/category/important-environment-variables">Important Environment Variables</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/category/c-introduction">C++ Introduction</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/category/ares-research-cluster">Ares User Guide</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Links</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/careers">GRC Careers</a></li><li class="footer__item"><a href="https://github.com/grc-iit" target="_blank" rel="noopener noreferrer" class="footer__link-item">GRC GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://www.linkedin.com/company/gnosis-research-center/" target="_blank" rel="noopener noreferrer" class="footer__link-item">GRC LinkedIn<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://twitter.com/grc_iit" target="_blank" rel="noopener noreferrer" class="footer__link-item">GRC X<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://www.youtube.com/channel/UCNtEyAt4_ckpIbX-gSv8sww" target="_blank" rel="noopener noreferrer" class="footer__link-item">GRC YouTube<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://www.iit.edu/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Illinois Tech<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="http://cs.iit.edu/~scs/" target="_blank" rel="noopener noreferrer" class="footer__link-item">SCS Lab<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Gnosis Research Center (GRC).</div></div></div></footer></div>
</body>
</html>